{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Live Audio Capture","text":"<p>Live Audio Capture is a cross-platform Python package designed for capturing, processing, and analyzing live audio from a microphone in real-time. Whether you're building a voice assistant, a transcription tool, or a real-time audio analysis application, this package has you covered.</p>"},{"location":"#features","title":"Features","text":"<ul> <li>Real-Time Audio Capture: Capture audio from the microphone in real-time.</li> <li>Voice Activity Detection (VAD): Automatically detect speech and stop recording during silence.</li> <li>Noise Reduction: Reduce background noise using advanced spectral gating techniques.</li> <li>Real-Time Visualization: Visualize audio waveforms, frequency spectra, and spectrograms.</li> <li>Customizable: Highly configurable parameters for sampling rate, chunk duration, noise reduction, and more.</li> <li>Cross-Platform: Works on Windows, macOS, and Linux.</li> </ul>"},{"location":"#quick-start","title":"Quick Start","text":"<p>To get started, install the package:</p> <pre><code>pip install live_audio_capture\n</code></pre> <p>Then, capture audio with voice activity detection:</p> <pre><code>from live_audio_capture import LiveAudioCapture\n\ncapture = LiveAudioCapture()\ncapture.listen_and_record_with_vad(output_file=\"output.wav\")\n</code></pre>"},{"location":"#explore-the-documentation","title":"Explore the Documentation","text":"<ul> <li>Installation: Learn how to install and set up the package.</li> <li>Usage: Discover how to use the package with examples.</li> <li>API Reference: Explore the full API documentation.</li> <li>Contributing: Find out how to contribute to the project.</li> </ul>"},{"location":"#support","title":"Support","text":"<p>If you have questions, issues, or feature requests, please open an issue on GitHub.</p>"},{"location":"#license","title":"License","text":"<p>This project is licensed under the MIT License. See the LICENSE file for details.</p>"},{"location":"api/","title":"API Reference","text":""},{"location":"api/#live_audio_capture","title":"<code>live_audio_capture</code>","text":""},{"location":"api/#live_audio_capture.AudioNoiseReduction","title":"<code>AudioNoiseReduction</code>","text":"<p>A utility class for audio processing tasks such as noise reduction, filtering, and resampling.</p> Source code in <code>live_audio_capture\\audio_noise_reduction.py</code> <pre><code>class AudioNoiseReduction:\n    \"\"\"\n    A utility class for audio processing tasks such as noise reduction, filtering, and resampling.\n    \"\"\"\n\n    @staticmethod\n    def apply_noise_reduction(\n        audio_chunk: np.ndarray,\n        sampling_rate: int,\n        stationary: bool = False,\n        prop_decrease: float = 1.0,\n        n_std_thresh_stationary: float = 1.5,\n        n_fft: int = 1024,\n        win_length: int = None,\n        hop_length: int = None,\n        n_jobs: int = 1,  # Number of parallel jobs\n        use_torch: bool = False,  # Use PyTorch for spectral gating\n        device: str = \"cuda\",  # Device for PyTorch computation\n    ) -&gt; np.ndarray:\n        \"\"\"\n        Apply noise reduction using the noisereduce package.\n\n        Args:\n            audio_chunk (np.ndarray): The audio chunk to process.\n            sampling_rate (int): The sample rate of the audio.\n            stationary (bool): Whether to perform stationary noise reduction.\n            prop_decrease (float): Proportion to reduce noise by (1.0 = 100%).\n            n_std_thresh_stationary (float): Number of standard deviations above mean for thresholding.\n            n_fft (int): FFT window size.\n            win_length (int): Window length for STFT.\n            hop_length (int): Hop length for STFT.\n            n_jobs (int): Number of parallel jobs to run. Set to -1 to use all CPU cores.\n            use_torch (bool): Whether to use the PyTorch version of spectral gating.\n            device (str): Device to run the PyTorch spectral gating on (e.g., \"cuda\" or \"cpu\").\n\n        Returns:\n            np.ndarray: The processed audio chunk with reduced noise.\n        \"\"\"\n        # Apply noise reduction using noisereduce\n        reduced_noise = nr.reduce_noise(\n            y=audio_chunk,\n            sr=sampling_rate,\n            stationary=stationary,\n            prop_decrease=prop_decrease,\n            n_std_thresh_stationary=n_std_thresh_stationary,\n            n_fft=n_fft,\n            win_length=win_length,\n            hop_length=hop_length,\n            n_jobs=n_jobs,  # Pass the number of parallel jobs\n            use_torch=use_torch,  # Enable/disable PyTorch\n            device=device,  # Specify the device for PyTorch\n        )\n        return reduced_noise\n\n    @staticmethod\n    def apply_low_pass_filter(\n        audio_chunk: np.ndarray,\n        sampling_rate: int,\n        cutoff_freq: float = 7900.0,  # Less than Nyquist frequency (8000 Hz)\n    ) -&gt; np.ndarray:\n        \"\"\"\n        Apply a low-pass filter to the audio chunk.\n\n        Args:\n            audio_chunk (np.ndarray): The audio chunk to process.\n            sampling_rate (int): The sample rate of the audio.\n            cutoff_freq (float): The cutoff frequency for the low-pass filter.\n\n        Returns:\n            np.ndarray: The filtered audio chunk.\n        \"\"\"\n        # Normalize the cutoff frequency to the range [0, 1]\n        nyquist = 0.5 * sampling_rate\n        if cutoff_freq &gt;= nyquist:\n            raise ValueError(\n                f\"Cutoff frequency must be less than the Nyquist frequency ({nyquist} Hz). \"\n                f\"Provided cutoff frequency: {cutoff_freq} Hz.\"\n            )\n        normal_cutoff = cutoff_freq / nyquist\n\n        # Design the Butterworth filter\n        b, a = butter(5, normal_cutoff, btype=\"low\", analog=False)\n\n        # Apply the filter to the audio chunk\n        return lfilter(b, a, audio_chunk)\n\n    @staticmethod\n    def resample_audio(\n        audio_chunk: np.ndarray,\n        original_rate: int,\n        target_rate: int,\n    ) -&gt; np.ndarray:\n        \"\"\"\n        Resample the audio chunk to a target sample rate.\n\n        Args:\n            audio_chunk (np.ndarray): The audio chunk to resample.\n            original_rate (int): The original sample rate.\n            target_rate (int): The target sample rate.\n\n        Returns:\n            np.ndarray: The resampled audio chunk.\n        \"\"\"\n        num_samples = int(len(audio_chunk) * target_rate / original_rate)\n        return resample(audio_chunk, num_samples)\n</code></pre>"},{"location":"api/#live_audio_capture.AudioNoiseReduction.apply_low_pass_filter","title":"<code>apply_low_pass_filter(audio_chunk, sampling_rate, cutoff_freq=7900.0)</code>  <code>staticmethod</code>","text":"<p>Apply a low-pass filter to the audio chunk.</p> <p>Parameters:</p> Name Type Description Default <code>audio_chunk</code> <code>ndarray</code> <p>The audio chunk to process.</p> required <code>sampling_rate</code> <code>int</code> <p>The sample rate of the audio.</p> required <code>cutoff_freq</code> <code>float</code> <p>The cutoff frequency for the low-pass filter.</p> <code>7900.0</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: The filtered audio chunk.</p> Source code in <code>live_audio_capture\\audio_noise_reduction.py</code> <pre><code>@staticmethod\ndef apply_low_pass_filter(\n    audio_chunk: np.ndarray,\n    sampling_rate: int,\n    cutoff_freq: float = 7900.0,  # Less than Nyquist frequency (8000 Hz)\n) -&gt; np.ndarray:\n    \"\"\"\n    Apply a low-pass filter to the audio chunk.\n\n    Args:\n        audio_chunk (np.ndarray): The audio chunk to process.\n        sampling_rate (int): The sample rate of the audio.\n        cutoff_freq (float): The cutoff frequency for the low-pass filter.\n\n    Returns:\n        np.ndarray: The filtered audio chunk.\n    \"\"\"\n    # Normalize the cutoff frequency to the range [0, 1]\n    nyquist = 0.5 * sampling_rate\n    if cutoff_freq &gt;= nyquist:\n        raise ValueError(\n            f\"Cutoff frequency must be less than the Nyquist frequency ({nyquist} Hz). \"\n            f\"Provided cutoff frequency: {cutoff_freq} Hz.\"\n        )\n    normal_cutoff = cutoff_freq / nyquist\n\n    # Design the Butterworth filter\n    b, a = butter(5, normal_cutoff, btype=\"low\", analog=False)\n\n    # Apply the filter to the audio chunk\n    return lfilter(b, a, audio_chunk)\n</code></pre>"},{"location":"api/#live_audio_capture.AudioNoiseReduction.apply_noise_reduction","title":"<code>apply_noise_reduction(audio_chunk, sampling_rate, stationary=False, prop_decrease=1.0, n_std_thresh_stationary=1.5, n_fft=1024, win_length=None, hop_length=None, n_jobs=1, use_torch=False, device='cuda')</code>  <code>staticmethod</code>","text":"<p>Apply noise reduction using the noisereduce package.</p> <p>Parameters:</p> Name Type Description Default <code>audio_chunk</code> <code>ndarray</code> <p>The audio chunk to process.</p> required <code>sampling_rate</code> <code>int</code> <p>The sample rate of the audio.</p> required <code>stationary</code> <code>bool</code> <p>Whether to perform stationary noise reduction.</p> <code>False</code> <code>prop_decrease</code> <code>float</code> <p>Proportion to reduce noise by (1.0 = 100%).</p> <code>1.0</code> <code>n_std_thresh_stationary</code> <code>float</code> <p>Number of standard deviations above mean for thresholding.</p> <code>1.5</code> <code>n_fft</code> <code>int</code> <p>FFT window size.</p> <code>1024</code> <code>win_length</code> <code>int</code> <p>Window length for STFT.</p> <code>None</code> <code>hop_length</code> <code>int</code> <p>Hop length for STFT.</p> <code>None</code> <code>n_jobs</code> <code>int</code> <p>Number of parallel jobs to run. Set to -1 to use all CPU cores.</p> <code>1</code> <code>use_torch</code> <code>bool</code> <p>Whether to use the PyTorch version of spectral gating.</p> <code>False</code> <code>device</code> <code>str</code> <p>Device to run the PyTorch spectral gating on (e.g., \"cuda\" or \"cpu\").</p> <code>'cuda'</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: The processed audio chunk with reduced noise.</p> Source code in <code>live_audio_capture\\audio_noise_reduction.py</code> <pre><code>@staticmethod\ndef apply_noise_reduction(\n    audio_chunk: np.ndarray,\n    sampling_rate: int,\n    stationary: bool = False,\n    prop_decrease: float = 1.0,\n    n_std_thresh_stationary: float = 1.5,\n    n_fft: int = 1024,\n    win_length: int = None,\n    hop_length: int = None,\n    n_jobs: int = 1,  # Number of parallel jobs\n    use_torch: bool = False,  # Use PyTorch for spectral gating\n    device: str = \"cuda\",  # Device for PyTorch computation\n) -&gt; np.ndarray:\n    \"\"\"\n    Apply noise reduction using the noisereduce package.\n\n    Args:\n        audio_chunk (np.ndarray): The audio chunk to process.\n        sampling_rate (int): The sample rate of the audio.\n        stationary (bool): Whether to perform stationary noise reduction.\n        prop_decrease (float): Proportion to reduce noise by (1.0 = 100%).\n        n_std_thresh_stationary (float): Number of standard deviations above mean for thresholding.\n        n_fft (int): FFT window size.\n        win_length (int): Window length for STFT.\n        hop_length (int): Hop length for STFT.\n        n_jobs (int): Number of parallel jobs to run. Set to -1 to use all CPU cores.\n        use_torch (bool): Whether to use the PyTorch version of spectral gating.\n        device (str): Device to run the PyTorch spectral gating on (e.g., \"cuda\" or \"cpu\").\n\n    Returns:\n        np.ndarray: The processed audio chunk with reduced noise.\n    \"\"\"\n    # Apply noise reduction using noisereduce\n    reduced_noise = nr.reduce_noise(\n        y=audio_chunk,\n        sr=sampling_rate,\n        stationary=stationary,\n        prop_decrease=prop_decrease,\n        n_std_thresh_stationary=n_std_thresh_stationary,\n        n_fft=n_fft,\n        win_length=win_length,\n        hop_length=hop_length,\n        n_jobs=n_jobs,  # Pass the number of parallel jobs\n        use_torch=use_torch,  # Enable/disable PyTorch\n        device=device,  # Specify the device for PyTorch\n    )\n    return reduced_noise\n</code></pre>"},{"location":"api/#live_audio_capture.AudioNoiseReduction.resample_audio","title":"<code>resample_audio(audio_chunk, original_rate, target_rate)</code>  <code>staticmethod</code>","text":"<p>Resample the audio chunk to a target sample rate.</p> <p>Parameters:</p> Name Type Description Default <code>audio_chunk</code> <code>ndarray</code> <p>The audio chunk to resample.</p> required <code>original_rate</code> <code>int</code> <p>The original sample rate.</p> required <code>target_rate</code> <code>int</code> <p>The target sample rate.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: The resampled audio chunk.</p> Source code in <code>live_audio_capture\\audio_noise_reduction.py</code> <pre><code>@staticmethod\ndef resample_audio(\n    audio_chunk: np.ndarray,\n    original_rate: int,\n    target_rate: int,\n) -&gt; np.ndarray:\n    \"\"\"\n    Resample the audio chunk to a target sample rate.\n\n    Args:\n        audio_chunk (np.ndarray): The audio chunk to resample.\n        original_rate (int): The original sample rate.\n        target_rate (int): The target sample rate.\n\n    Returns:\n        np.ndarray: The resampled audio chunk.\n    \"\"\"\n    num_samples = int(len(audio_chunk) * target_rate / original_rate)\n    return resample(audio_chunk, num_samples)\n</code></pre>"},{"location":"api/#live_audio_capture.AudioPlayback","title":"<code>AudioPlayback</code>","text":"<p>Utilities for playing audio files and sounds.</p> Source code in <code>live_audio_capture\\audio_utils\\audio_playback.py</code> <pre><code>class AudioPlayback:\n    \"\"\"\n    Utilities for playing audio files and sounds.\n    \"\"\"\n\n    @staticmethod\n    def play_audio_file(file_path: str) -&gt; None:\n        \"\"\"\n        Play an audio file using the simpleaudio library.\n\n        Args:\n            file_path (str): Path to the audio file to play.\n        \"\"\"\n        try:\n            audio = AudioSegment.from_file(file_path)\n            raw_data = audio.raw_data\n            play_obj = sa.play_buffer(raw_data, num_channels=audio.channels, bytes_per_sample=audio.sample_width, sample_rate=audio.frame_rate)\n            play_obj.wait_done()\n        except Exception as e:\n            print(f\"Failed to play audio file: {e}\")\n\n    @staticmethod\n    def play_beep(frequency: int, duration: int) -&gt; None:\n        \"\"\"\n        Play a beep sound asynchronously using the simpleaudio library.\n\n        Args:\n            frequency (int): Frequency of the beep sound in Hz.\n            duration (int): Duration of the beep sound in milliseconds.\n        \"\"\"\n        try:\n            sample_rate = 44100\n            t = np.linspace(0, duration / 1000, int(sample_rate * duration / 1000), endpoint=False)\n            waveform = np.sin(2 * np.pi * frequency * t)\n            waveform = (waveform * 32767).astype(np.int16)\n            play_obj = sa.play_buffer(waveform, num_channels=1, bytes_per_sample=2, sample_rate=sample_rate)\n            play_obj.stop()\n        except Exception as e:\n            print(f\"Failed to play beep sound: {e}\")\n</code></pre>"},{"location":"api/#live_audio_capture.AudioPlayback.play_audio_file","title":"<code>play_audio_file(file_path)</code>  <code>staticmethod</code>","text":"<p>Play an audio file using the simpleaudio library.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>Path to the audio file to play.</p> required Source code in <code>live_audio_capture\\audio_utils\\audio_playback.py</code> <pre><code>@staticmethod\ndef play_audio_file(file_path: str) -&gt; None:\n    \"\"\"\n    Play an audio file using the simpleaudio library.\n\n    Args:\n        file_path (str): Path to the audio file to play.\n    \"\"\"\n    try:\n        audio = AudioSegment.from_file(file_path)\n        raw_data = audio.raw_data\n        play_obj = sa.play_buffer(raw_data, num_channels=audio.channels, bytes_per_sample=audio.sample_width, sample_rate=audio.frame_rate)\n        play_obj.wait_done()\n    except Exception as e:\n        print(f\"Failed to play audio file: {e}\")\n</code></pre>"},{"location":"api/#live_audio_capture.AudioPlayback.play_beep","title":"<code>play_beep(frequency, duration)</code>  <code>staticmethod</code>","text":"<p>Play a beep sound asynchronously using the simpleaudio library.</p> <p>Parameters:</p> Name Type Description Default <code>frequency</code> <code>int</code> <p>Frequency of the beep sound in Hz.</p> required <code>duration</code> <code>int</code> <p>Duration of the beep sound in milliseconds.</p> required Source code in <code>live_audio_capture\\audio_utils\\audio_playback.py</code> <pre><code>@staticmethod\ndef play_beep(frequency: int, duration: int) -&gt; None:\n    \"\"\"\n    Play a beep sound asynchronously using the simpleaudio library.\n\n    Args:\n        frequency (int): Frequency of the beep sound in Hz.\n        duration (int): Duration of the beep sound in milliseconds.\n    \"\"\"\n    try:\n        sample_rate = 44100\n        t = np.linspace(0, duration / 1000, int(sample_rate * duration / 1000), endpoint=False)\n        waveform = np.sin(2 * np.pi * frequency * t)\n        waveform = (waveform * 32767).astype(np.int16)\n        play_obj = sa.play_buffer(waveform, num_channels=1, bytes_per_sample=2, sample_rate=sample_rate)\n        play_obj.stop()\n    except Exception as e:\n        print(f\"Failed to play beep sound: {e}\")\n</code></pre>"},{"location":"api/#live_audio_capture.AudioProcessing","title":"<code>AudioProcessing</code>","text":"<p>Utilities for processing audio data.</p> Source code in <code>live_audio_capture\\audio_utils\\audio_processing.py</code> <pre><code>class AudioProcessing:\n    \"\"\"\n    Utilities for processing audio data.\n    \"\"\"\n\n    @staticmethod\n    def calculate_energy(audio_chunk: np.ndarray) -&gt; float:\n        \"\"\"\n        Calculate the energy of an audio chunk.\n\n        Args:\n            audio_chunk (np.ndarray): The audio chunk to process.\n\n        Returns:\n            float: The energy of the audio chunk.\n        \"\"\"\n        return np.sum(audio_chunk**2) / len(audio_chunk)\n\n    @staticmethod\n    def process_audio_chunk(raw_data: bytes, audio_format: str = \"f32le\") -&gt; np.ndarray:\n        \"\"\"\n        Convert raw audio data to a NumPy array based on the audio format.\n\n        Args:\n            raw_data (bytes): Raw audio data from the microphone.\n            audio_format (str): Audio format (e.g., \"f32le\" or \"s16le\").\n\n        Returns:\n            np.ndarray: The processed audio chunk.\n        \"\"\"\n        if audio_format == \"f32le\":\n            return np.frombuffer(raw_data, dtype=np.float32)\n        elif audio_format == \"s16le\":\n            return np.frombuffer(raw_data, dtype=np.int16) / 32768.0  # Normalize to [-1, 1]\n        else:\n            raise ValueError(f\"Unsupported audio format: {audio_format}\")\n\n    @staticmethod\n    def apply_noise_reduction_to_file(\n        input_file: str,\n        output_file: str,\n        stationary: bool = False,\n        prop_decrease: float = 1.0,\n        n_std_thresh_stationary: float = 1.5,\n        n_jobs: int = 1,\n        use_torch: bool = False,\n        device: str = \"cuda\",\n    ) -&gt; None:\n        \"\"\"\n        Apply noise reduction to an audio file and save the result.\n\n        Args:\n            input_file (str): Path to the input audio file.\n            output_file (str): Path to save the processed audio file.\n            stationary (bool): Whether to perform stationary noise reduction.\n            prop_decrease (float): Proportion to reduce noise by (1.0 = 100%).\n            n_std_thresh_stationary (float): Threshold for stationary noise reduction.\n            n_jobs (int): Number of parallel jobs to run. Set to -1 to use all CPU cores.\n            use_torch (bool): Whether to use the PyTorch version of spectral gating.\n            device (str): Device to run the PyTorch spectral gating on (e.g., \"cuda\" or \"cpu\").\n        \"\"\"\n        try:\n            # Load the audio file using pydub\n            audio = AudioSegment.from_file(input_file)\n\n            # Convert the audio to a NumPy array\n            samples = np.array(audio.get_array_of_samples())\n            sample_rate = audio.frame_rate\n\n            # Normalize the audio to the range [-1, 1]\n            if audio.sample_width == 2:  # 16-bit audio\n                samples = samples / 32768.0\n            elif audio.sample_width == 4:  # 32-bit audio\n                samples = samples / 2147483648.0\n\n            # Apply noise reduction\n            reduced_noise = nr.reduce_noise(\n                y=samples,\n                sr=sample_rate,\n                stationary=stationary,\n                prop_decrease=prop_decrease,\n                n_std_thresh_stationary=n_std_thresh_stationary,\n                n_jobs=n_jobs,\n                use_torch=use_torch,\n                device=device,\n            )\n\n            # Scale the audio back to the original range\n            if audio.sample_width == 2:  # 16-bit audio\n                reduced_noise = (reduced_noise * 32768.0).astype(np.int16)\n            elif audio.sample_width == 4:  # 32-bit audio\n                reduced_noise = (reduced_noise * 2147483648.0).astype(np.int32)\n\n            # Convert the NumPy array back to an AudioSegment\n            processed_audio = AudioSegment(\n                reduced_noise.tobytes(),\n                frame_rate=sample_rate,\n                sample_width=audio.sample_width,\n                channels=audio.channels,\n            )\n\n            # Save the processed audio to the output file\n            processed_audio.export(output_file, format=output_file.split(\".\")[-1])\n            print(f\"Noise-reduced audio saved to {output_file}\")\n\n        except Exception as e:\n            print(f\"Failed to apply noise reduction to audio file: {e}\")\n</code></pre>"},{"location":"api/#live_audio_capture.AudioProcessing.apply_noise_reduction_to_file","title":"<code>apply_noise_reduction_to_file(input_file, output_file, stationary=False, prop_decrease=1.0, n_std_thresh_stationary=1.5, n_jobs=1, use_torch=False, device='cuda')</code>  <code>staticmethod</code>","text":"<p>Apply noise reduction to an audio file and save the result.</p> <p>Parameters:</p> Name Type Description Default <code>input_file</code> <code>str</code> <p>Path to the input audio file.</p> required <code>output_file</code> <code>str</code> <p>Path to save the processed audio file.</p> required <code>stationary</code> <code>bool</code> <p>Whether to perform stationary noise reduction.</p> <code>False</code> <code>prop_decrease</code> <code>float</code> <p>Proportion to reduce noise by (1.0 = 100%).</p> <code>1.0</code> <code>n_std_thresh_stationary</code> <code>float</code> <p>Threshold for stationary noise reduction.</p> <code>1.5</code> <code>n_jobs</code> <code>int</code> <p>Number of parallel jobs to run. Set to -1 to use all CPU cores.</p> <code>1</code> <code>use_torch</code> <code>bool</code> <p>Whether to use the PyTorch version of spectral gating.</p> <code>False</code> <code>device</code> <code>str</code> <p>Device to run the PyTorch spectral gating on (e.g., \"cuda\" or \"cpu\").</p> <code>'cuda'</code> Source code in <code>live_audio_capture\\audio_utils\\audio_processing.py</code> <pre><code>@staticmethod\ndef apply_noise_reduction_to_file(\n    input_file: str,\n    output_file: str,\n    stationary: bool = False,\n    prop_decrease: float = 1.0,\n    n_std_thresh_stationary: float = 1.5,\n    n_jobs: int = 1,\n    use_torch: bool = False,\n    device: str = \"cuda\",\n) -&gt; None:\n    \"\"\"\n    Apply noise reduction to an audio file and save the result.\n\n    Args:\n        input_file (str): Path to the input audio file.\n        output_file (str): Path to save the processed audio file.\n        stationary (bool): Whether to perform stationary noise reduction.\n        prop_decrease (float): Proportion to reduce noise by (1.0 = 100%).\n        n_std_thresh_stationary (float): Threshold for stationary noise reduction.\n        n_jobs (int): Number of parallel jobs to run. Set to -1 to use all CPU cores.\n        use_torch (bool): Whether to use the PyTorch version of spectral gating.\n        device (str): Device to run the PyTorch spectral gating on (e.g., \"cuda\" or \"cpu\").\n    \"\"\"\n    try:\n        # Load the audio file using pydub\n        audio = AudioSegment.from_file(input_file)\n\n        # Convert the audio to a NumPy array\n        samples = np.array(audio.get_array_of_samples())\n        sample_rate = audio.frame_rate\n\n        # Normalize the audio to the range [-1, 1]\n        if audio.sample_width == 2:  # 16-bit audio\n            samples = samples / 32768.0\n        elif audio.sample_width == 4:  # 32-bit audio\n            samples = samples / 2147483648.0\n\n        # Apply noise reduction\n        reduced_noise = nr.reduce_noise(\n            y=samples,\n            sr=sample_rate,\n            stationary=stationary,\n            prop_decrease=prop_decrease,\n            n_std_thresh_stationary=n_std_thresh_stationary,\n            n_jobs=n_jobs,\n            use_torch=use_torch,\n            device=device,\n        )\n\n        # Scale the audio back to the original range\n        if audio.sample_width == 2:  # 16-bit audio\n            reduced_noise = (reduced_noise * 32768.0).astype(np.int16)\n        elif audio.sample_width == 4:  # 32-bit audio\n            reduced_noise = (reduced_noise * 2147483648.0).astype(np.int32)\n\n        # Convert the NumPy array back to an AudioSegment\n        processed_audio = AudioSegment(\n            reduced_noise.tobytes(),\n            frame_rate=sample_rate,\n            sample_width=audio.sample_width,\n            channels=audio.channels,\n        )\n\n        # Save the processed audio to the output file\n        processed_audio.export(output_file, format=output_file.split(\".\")[-1])\n        print(f\"Noise-reduced audio saved to {output_file}\")\n\n    except Exception as e:\n        print(f\"Failed to apply noise reduction to audio file: {e}\")\n</code></pre>"},{"location":"api/#live_audio_capture.AudioProcessing.calculate_energy","title":"<code>calculate_energy(audio_chunk)</code>  <code>staticmethod</code>","text":"<p>Calculate the energy of an audio chunk.</p> <p>Parameters:</p> Name Type Description Default <code>audio_chunk</code> <code>ndarray</code> <p>The audio chunk to process.</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>The energy of the audio chunk.</p> Source code in <code>live_audio_capture\\audio_utils\\audio_processing.py</code> <pre><code>@staticmethod\ndef calculate_energy(audio_chunk: np.ndarray) -&gt; float:\n    \"\"\"\n    Calculate the energy of an audio chunk.\n\n    Args:\n        audio_chunk (np.ndarray): The audio chunk to process.\n\n    Returns:\n        float: The energy of the audio chunk.\n    \"\"\"\n    return np.sum(audio_chunk**2) / len(audio_chunk)\n</code></pre>"},{"location":"api/#live_audio_capture.AudioProcessing.process_audio_chunk","title":"<code>process_audio_chunk(raw_data, audio_format='f32le')</code>  <code>staticmethod</code>","text":"<p>Convert raw audio data to a NumPy array based on the audio format.</p> <p>Parameters:</p> Name Type Description Default <code>raw_data</code> <code>bytes</code> <p>Raw audio data from the microphone.</p> required <code>audio_format</code> <code>str</code> <p>Audio format (e.g., \"f32le\" or \"s16le\").</p> <code>'f32le'</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: The processed audio chunk.</p> Source code in <code>live_audio_capture\\audio_utils\\audio_processing.py</code> <pre><code>@staticmethod\ndef process_audio_chunk(raw_data: bytes, audio_format: str = \"f32le\") -&gt; np.ndarray:\n    \"\"\"\n    Convert raw audio data to a NumPy array based on the audio format.\n\n    Args:\n        raw_data (bytes): Raw audio data from the microphone.\n        audio_format (str): Audio format (e.g., \"f32le\" or \"s16le\").\n\n    Returns:\n        np.ndarray: The processed audio chunk.\n    \"\"\"\n    if audio_format == \"f32le\":\n        return np.frombuffer(raw_data, dtype=np.float32)\n    elif audio_format == \"s16le\":\n        return np.frombuffer(raw_data, dtype=np.int16) / 32768.0  # Normalize to [-1, 1]\n    else:\n        raise ValueError(f\"Unsupported audio format: {audio_format}\")\n</code></pre>"},{"location":"api/#live_audio_capture.AudioVisualizer","title":"<code>AudioVisualizer</code>","text":"<p>A standalone real-time audio visualizer using PyQtGraph with a refined color scheme.</p> Source code in <code>live_audio_capture\\visualization.py</code> <pre><code>class AudioVisualizer:\n    \"\"\"\n    A standalone real-time audio visualizer using PyQtGraph with a refined color scheme.\n    \"\"\"\n\n    def __init__(self, sampling_rate: int, chunk_duration: float):\n        \"\"\"\n        Initialize the AudioVisualizer instance.\n\n        Args:\n            sampling_rate (int): The sample rate of the audio.\n            chunk_duration (float): The duration of each audio chunk in seconds.\n        \"\"\"\n        self.sampling_rate = sampling_rate\n        self.chunk_duration = chunk_duration\n        self.chunk_size = int(sampling_rate * chunk_duration)\n\n        # Create a thread-safe queue for audio chunks\n        self.audio_queue = queue.Queue()\n\n        # Flag to control the visualization thread\n        self.running = False\n\n        # Start the visualization thread\n        self.thread = Thread(target=self._run_visualizer, daemon=True)\n        self.thread.start()\n\n    def _run_visualizer(self):\n        \"\"\"Run the PyQtGraph visualizer in a separate thread.\"\"\"\n        # Create a PyQtGraph application\n        self.app = QtWidgets.QApplication([])\n        self.win = pg.GraphicsLayoutWidget(title=\"Real-Time Audio Visualizer\")\n        self.win.resize(1200, 800)  # Increase window size for additional plots\n        self.win.setBackground(\"#1f1f1f\")  # Dark gray background\n        self.win.show()\n\n        # Custom font for labels\n        _ = QtGui.QFont(\"Arial\", 12)\n        pg.setConfigOptions(antialias=True, useNumba=True)\n\n        # Create a plot for the waveform\n        self.waveform_plot = self.win.addPlot(title=\"Waveform\", row=0, col=0)\n        self.waveform_curve = self.waveform_plot.plot(pen=pg.mkPen(\"#00ffff\", width=2))  # Soft cyan\n        self.waveform_plot.setYRange(-1, 1)\n        self.waveform_plot.setXRange(0, self.chunk_size)\n        self.waveform_plot.setTitle(\"Waveform\", color=\"#ffffff\", size=\"14pt\")\n        self.waveform_plot.setLabel(\"left\", \"Amplitude\", color=\"#ffffff\", **{\"font-size\": \"12pt\"})\n        self.waveform_plot.setLabel(\"bottom\", \"Time (samples)\", color=\"#ffffff\", **{\"font-size\": \"12pt\"})\n\n        # Create a plot for the frequency spectrum\n        self.spectrum_plot = self.win.addPlot(title=\"Frequency Spectrum\", row=0, col=1)\n        self.spectrum_curve = self.spectrum_plot.plot(pen=pg.mkPen(\"#ff00ff\", width=2))  # Soft magenta\n        self.spectrum_plot.setLogMode(x=True, y=False)  # Logarithmic frequency axis\n        self.spectrum_plot.setLabel(\"left\", \"Magnitude (dB)\", color=\"#ffffff\", **{\"font-size\": \"12pt\"})\n        self.spectrum_plot.setLabel(\"bottom\", \"Frequency (Hz)\", color=\"#ffffff\", **{\"font-size\": \"12pt\"})\n        self.spectrum_plot.setYRange(-100, 0)\n        self.spectrum_plot.setXRange(20, self.sampling_rate / 2)  # 20 Hz to Nyquist frequency\n        self.spectrum_plot.setTitle(\"Frequency Spectrum\", color=\"#ffffff\", size=\"14pt\")\n\n        # Add a peak frequency indicator\n        self.peak_freq_text = pg.TextItem(anchor=(0.5, 1), color=\"#ff00ff\")  # Soft magenta\n        self.spectrum_plot.addItem(self.peak_freq_text)\n\n        # Create a plot for the spectrogram\n        self.spectrogram_plot = self.win.addPlot(title=\"Spectrogram\", row=1, col=0, colspan=2)\n        self.spectrogram_image = pg.ImageItem()\n        self.spectrogram_plot.addItem(self.spectrogram_image)\n        self.spectrogram_plot.setLabel(\"left\", \"Frequency (Hz)\", color=\"#ffffff\", **{\"font-size\": \"12pt\"})\n        self.spectrogram_plot.setLabel(\"bottom\", \"Time (s)\", color=\"#ffffff\", **{\"font-size\": \"12pt\"})\n        self.spectrogram_plot.setTitle(\"Spectrogram\", color=\"#ffffff\", size=\"14pt\")\n\n        # Set a color map for the spectrogram (deep blue to bright yellow)\n        self.colormap = pg.ColorMap(\n            [0.0, 1.0],  # Positions for the colors\n            [\n                (0, 0, 255),  # Deep blue at position 0.0\n                (255, 255, 0),  # Bright yellow at position 1.0\n            ]\n        )\n        self.spectrogram_image.setLookupTable(self.colormap.getLookupTable())\n\n        # Initialize spectrogram data\n        self.spectrogram_data = np.zeros((129, 100))  # 129 frequency bins, 100 time steps\n        self.spectrogram_image.setImage(self.spectrogram_data)\n        self.spectrogram_image.setLevels([-50, 0])  # Adjust levels for better contrast\n\n        # Create a volume meter\n        self.volume_meter = self.win.addPlot(title=\"Volume Meter\", row=2, col=0)\n        self.volume_bar = pg.BarGraphItem(x=[0], height=[0], width=0.6, brush=\"#ff8c42\")  # Gradient orange\n        self.volume_meter.addItem(self.volume_bar)\n        self.volume_meter.setYRange(0, 1)\n        self.volume_meter.setXRange(-1, 1)\n        self.volume_meter.setTitle(\"Volume Meter\", color=\"#ffffff\", size=\"14pt\")\n        self.volume_meter.setLabel(\"left\", \"Volume\", color=\"#ffffff\", **{\"font-size\": \"12pt\"})\n\n        # Create a volume history plot\n        self.volume_history_plot = self.win.addPlot(title=\"Volume History\", row=2, col=1)\n        self.volume_history_curve = self.volume_history_plot.plot(pen=pg.mkPen(\"#00ff00\", width=2))  # Soft green\n        self.volume_history_plot.setYRange(0, 1)\n        self.volume_history_plot.setXRange(0, 100)  # Show last 100 volume readings\n        self.volume_history_plot.setTitle(\"Volume History\", color=\"#ffffff\", size=\"14pt\")\n        self.volume_history_plot.setLabel(\"left\", \"Volume\", color=\"#ffffff\", **{\"font-size\": \"12pt\"})\n        self.volume_history_plot.setLabel(\"bottom\", \"Time (samples)\", color=\"#ffffff\", **{\"font-size\": \"12pt\"})\n\n        # Timer for updating the visualization\n        self.timer = QtCore.QTimer()\n        self.timer.timeout.connect(self._update)\n        self.timer.start(100)  # Update every 100 ms\n\n        # Start the Qt event loop\n        self.running = True\n        self.app.exec()\n\n    def _update(self):\n        \"\"\"Update the visualization with the latest audio chunk.\"\"\"\n        try:\n            # Get the latest audio chunk from the queue\n            audio_chunk = self.audio_queue.get_nowait()\n\n            # Update waveform\n            self.waveform_curve.setData(audio_chunk)\n\n            # Update frequency spectrum\n            spectrum = self.compute_spectrum(audio_chunk)\n            if spectrum is not None:\n                freqs = np.fft.rfftfreq(len(audio_chunk), 1 / self.sampling_rate)\n                self.spectrum_curve.setData(freqs, spectrum)\n\n                # Update peak frequency indicator\n                peak_freq = freqs[np.argmax(spectrum)]\n                self.peak_freq_text.setText(f\"Peak: {peak_freq:.1f} Hz\")\n                self.peak_freq_text.setPos(peak_freq, np.max(spectrum))\n\n            # Update spectrogram\n            spectrogram_chunk = self.compute_spectrogram(audio_chunk)\n            if spectrogram_chunk is not None:\n                self.spectrogram_data = np.roll(self.spectrogram_data, -1, axis=1)\n                self.spectrogram_data[:, -1] = spectrogram_chunk\n                self.spectrogram_image.setImage(self.spectrogram_data, autoLevels=False)\n\n            # Update volume meter\n            volume = np.sqrt(np.mean(audio_chunk**2))  # RMS volume\n            self.volume_bar.setOpts(height=[volume])\n\n            # Update volume history\n            if not hasattr(self, \"volume_history\"):\n                self.volume_history = np.zeros(100)\n            self.volume_history = np.roll(self.volume_history, -1)\n            self.volume_history[-1] = volume\n            self.volume_history_curve.setData(self.volume_history)\n\n        except queue.Empty:\n            # No new audio chunk available\n            pass\n\n    def compute_spectrum(self, audio_chunk: np.ndarray) -&gt; Optional[np.ndarray]:\n        \"\"\"\n        Compute the frequency spectrum for a given audio chunk.\n\n        Args:\n            audio_chunk (np.ndarray): The audio chunk to process.\n\n        Returns:\n            Optional[np.ndarray]: The frequency spectrum in dB.\n        \"\"\"\n        try:\n            # Compute the FFT\n            fft = np.fft.rfft(audio_chunk)\n            magnitude = np.abs(fft)\n            return 10 * np.log10(magnitude + 1e-10)  # Convert to dB\n        except Exception:\n            return None\n\n    def compute_spectrogram(self, audio_chunk: np.ndarray) -&gt; Optional[np.ndarray]:\n        \"\"\"\n        Compute the spectrogram for a given audio chunk.\n\n        Args:\n            audio_chunk (np.ndarray): The audio chunk to process.\n\n        Returns:\n            Optional[np.ndarray]: The spectrogram data.\n        \"\"\"\n        try:\n            _, _, Sxx = spectrogram(audio_chunk, fs=self.sampling_rate, nperseg=256)\n            return 10 * np.log10(Sxx.mean(axis=1) + 1e-10)  # Convert to dB\n        except ValueError:\n            # Handle cases where the audio chunk is too short for the spectrogram\n            return None\n\n    def add_audio_chunk(self, audio_chunk: np.ndarray):\n        \"\"\"Add a new audio chunk to the visualization queue.\"\"\"\n        self.audio_queue.put(audio_chunk)\n\n    def stop(self):\n        \"\"\"Stop the visualization.\"\"\"\n        self.running = False\n        self.app.quit()\n</code></pre>"},{"location":"api/#live_audio_capture.AudioVisualizer.__init__","title":"<code>__init__(sampling_rate, chunk_duration)</code>","text":"<p>Initialize the AudioVisualizer instance.</p> <p>Parameters:</p> Name Type Description Default <code>sampling_rate</code> <code>int</code> <p>The sample rate of the audio.</p> required <code>chunk_duration</code> <code>float</code> <p>The duration of each audio chunk in seconds.</p> required Source code in <code>live_audio_capture\\visualization.py</code> <pre><code>def __init__(self, sampling_rate: int, chunk_duration: float):\n    \"\"\"\n    Initialize the AudioVisualizer instance.\n\n    Args:\n        sampling_rate (int): The sample rate of the audio.\n        chunk_duration (float): The duration of each audio chunk in seconds.\n    \"\"\"\n    self.sampling_rate = sampling_rate\n    self.chunk_duration = chunk_duration\n    self.chunk_size = int(sampling_rate * chunk_duration)\n\n    # Create a thread-safe queue for audio chunks\n    self.audio_queue = queue.Queue()\n\n    # Flag to control the visualization thread\n    self.running = False\n\n    # Start the visualization thread\n    self.thread = Thread(target=self._run_visualizer, daemon=True)\n    self.thread.start()\n</code></pre>"},{"location":"api/#live_audio_capture.AudioVisualizer.add_audio_chunk","title":"<code>add_audio_chunk(audio_chunk)</code>","text":"<p>Add a new audio chunk to the visualization queue.</p> Source code in <code>live_audio_capture\\visualization.py</code> <pre><code>def add_audio_chunk(self, audio_chunk: np.ndarray):\n    \"\"\"Add a new audio chunk to the visualization queue.\"\"\"\n    self.audio_queue.put(audio_chunk)\n</code></pre>"},{"location":"api/#live_audio_capture.AudioVisualizer.compute_spectrogram","title":"<code>compute_spectrogram(audio_chunk)</code>","text":"<p>Compute the spectrogram for a given audio chunk.</p> <p>Parameters:</p> Name Type Description Default <code>audio_chunk</code> <code>ndarray</code> <p>The audio chunk to process.</p> required <p>Returns:</p> Type Description <code>Optional[ndarray]</code> <p>Optional[np.ndarray]: The spectrogram data.</p> Source code in <code>live_audio_capture\\visualization.py</code> <pre><code>def compute_spectrogram(self, audio_chunk: np.ndarray) -&gt; Optional[np.ndarray]:\n    \"\"\"\n    Compute the spectrogram for a given audio chunk.\n\n    Args:\n        audio_chunk (np.ndarray): The audio chunk to process.\n\n    Returns:\n        Optional[np.ndarray]: The spectrogram data.\n    \"\"\"\n    try:\n        _, _, Sxx = spectrogram(audio_chunk, fs=self.sampling_rate, nperseg=256)\n        return 10 * np.log10(Sxx.mean(axis=1) + 1e-10)  # Convert to dB\n    except ValueError:\n        # Handle cases where the audio chunk is too short for the spectrogram\n        return None\n</code></pre>"},{"location":"api/#live_audio_capture.AudioVisualizer.compute_spectrum","title":"<code>compute_spectrum(audio_chunk)</code>","text":"<p>Compute the frequency spectrum for a given audio chunk.</p> <p>Parameters:</p> Name Type Description Default <code>audio_chunk</code> <code>ndarray</code> <p>The audio chunk to process.</p> required <p>Returns:</p> Type Description <code>Optional[ndarray]</code> <p>Optional[np.ndarray]: The frequency spectrum in dB.</p> Source code in <code>live_audio_capture\\visualization.py</code> <pre><code>def compute_spectrum(self, audio_chunk: np.ndarray) -&gt; Optional[np.ndarray]:\n    \"\"\"\n    Compute the frequency spectrum for a given audio chunk.\n\n    Args:\n        audio_chunk (np.ndarray): The audio chunk to process.\n\n    Returns:\n        Optional[np.ndarray]: The frequency spectrum in dB.\n    \"\"\"\n    try:\n        # Compute the FFT\n        fft = np.fft.rfft(audio_chunk)\n        magnitude = np.abs(fft)\n        return 10 * np.log10(magnitude + 1e-10)  # Convert to dB\n    except Exception:\n        return None\n</code></pre>"},{"location":"api/#live_audio_capture.AudioVisualizer.stop","title":"<code>stop()</code>","text":"<p>Stop the visualization.</p> Source code in <code>live_audio_capture\\visualization.py</code> <pre><code>def stop(self):\n    \"\"\"Stop the visualization.\"\"\"\n    self.running = False\n    self.app.quit()\n</code></pre>"},{"location":"api/#live_audio_capture.LiveAudioCapture","title":"<code>LiveAudioCapture</code>","text":"<p>A cross-platform utility for capturing live audio from a microphone using FFmpeg. Features: - Continuous listening mode. - Dynamic recording based on voice activity. - Silence duration threshold for stopping recording. - Optional beep sounds for start/stop feedback. - Save recordings in multiple formats (WAV, MP3, OGG).</p> Source code in <code>live_audio_capture\\audio_capture.py</code> <pre><code>class LiveAudioCapture:\n    \"\"\"\n    A cross-platform utility for capturing live audio from a microphone using FFmpeg.\n    Features:\n    - Continuous listening mode.\n    - Dynamic recording based on voice activity.\n    - Silence duration threshold for stopping recording.\n    - Optional beep sounds for start/stop feedback.\n    - Save recordings in multiple formats (WAV, MP3, OGG).\n    \"\"\"\n\n    def __init__(\n        self,\n        sampling_rate: int = 16000,\n        chunk_duration: float = 0.1,\n        audio_format: str = \"f32le\",\n        channels: int = 1,\n        aggressiveness: int = 1,  # Aggressiveness level for VAD\n        enable_beep: bool = True,\n        enable_noise_canceling: bool = False,\n        low_pass_cutoff: float = 7500.0,\n        stationary_noise_reduction: bool = False,\n        prop_decrease: float = 1.0,\n        n_std_thresh_stationary: float = 1.5,\n        n_jobs: int = 1,\n        use_torch: bool = False,\n        device: str = \"cuda\",\n        calibration_duration: float = 2.0,  # Duration of calibration in seconds\n        use_adaptive_threshold: bool = True,  # Enable adaptive thresholding\n    ):\n        \"\"\"\n        Initialize the LiveAudioCapture instance.\n\n        Args:\n            sampling_rate (int): Sample rate in Hz (e.g., 16000).\n            chunk_duration (float): Duration of each audio chunk in seconds (e.g., 0.1).\n            audio_format (str): Audio format for FFmpeg output (e.g., \"f32le\").\n            channels (int): Number of audio channels (1 for mono, 2 for stereo).\n            aggressiveness (int): Aggressiveness level for VAD (0 = least aggressive, 3 = most aggressive).\n            enable_beep (bool): Whether to play beep sounds when recording starts/stops.\n            enable_noise_canceling (bool): Whether to apply noise cancellation.\n            low_pass_cutoff (float): Cutoff frequency for the low-pass filter.\n            stationary_noise_reduction (bool): Whether to use stationary noise reduction.\n            prop_decrease (float): Proportion to reduce noise by (1.0 = 100%).\n            n_std_thresh_stationary (float): Threshold for stationary noise reduction.\n            n_jobs (int): Number of parallel jobs to run. Set to -1 to use all CPU cores.\n            use_torch (bool): Whether to use the PyTorch version of spectral gating.\n            device (str): Device to run the PyTorch spectral gating on (e.g., \"cuda\" or \"cpu\").\n            calibration_duration (float): Duration of the calibration phase in seconds.\n            use_adaptive_threshold (bool): Whether to use adaptive thresholding for VAD.\n        \"\"\"\n        self.sampling_rate = sampling_rate\n        self.chunk_duration = chunk_duration\n        self.audio_format = audio_format\n        self.channels = channels\n        self.enable_beep = enable_beep\n        self.enable_noise_canceling = enable_noise_canceling\n        self.low_pass_cutoff = low_pass_cutoff\n        self.stationary_noise_reduction = stationary_noise_reduction\n        self.prop_decrease = prop_decrease\n        self.n_std_thresh_stationary = n_std_thresh_stationary\n        self.n_jobs = n_jobs\n        self.use_torch = use_torch\n        self.device = device\n        self.calibration_duration = calibration_duration\n        self.use_adaptive_threshold = use_adaptive_threshold\n        self.process: Optional[subprocess.Popen] = None\n        self.is_streaming = False\n        self.is_recording = False\n\n        # Validate the cutoff frequency\n        nyquist = 0.5 * self.sampling_rate\n        if self.low_pass_cutoff &gt;= nyquist:\n            raise ValueError(\n                f\"Cutoff frequency must be less than the Nyquist frequency ({nyquist} Hz). \"\n                f\"Provided cutoff frequency: {self.low_pass_cutoff} Hz.\"\n            )\n\n        # Initialize VAD\n        self.vad = VoiceActivityDetector(\n            sample_rate=sampling_rate,\n            frame_duration=chunk_duration,\n            aggressiveness=aggressiveness,\n            hysteresis_high=1.5,\n            hysteresis_low=0.5,\n            enable_noise_canceling=self.enable_noise_canceling,\n            calibration_duration=self.calibration_duration,\n            use_adaptive_threshold=self.use_adaptive_threshold,\n            audio_format=self.audio_format,\n            channels=self.channels\n        )\n\n        # Determine the input device based on the platform\n        if sys.platform == \"linux\":\n            self.input_format = \"alsa\"\n        elif sys.platform == \"darwin\":  # macOS\n            self.input_format = \"avfoundation\"\n        elif sys.platform == \"win32\":\n            self.input_format = \"dshow\"\n        else:\n            raise RuntimeError(f\"Unsupported platform: {sys.platform}\")\n\n        # Get the default microphone\n        self.input_device = MicUtils.get_default_mic()\n        print(f\"Using input device: {self.input_device}\")\n\n    def list_available_mics(self) -&gt; Dict[str, str]:\n        \"\"\"\n        List all available microphones on the system.\n\n        Returns:\n            Dict[str, str]: A dictionary mapping microphone names to their device IDs.\n        \"\"\"\n        return MicUtils.list_mics()\n\n    def change_input_device(self, mic_name: str) -&gt; None:\n        \"\"\"\n        Change the input device to the specified microphone by name.\n\n        Args:\n            mic_name (str): The name of the microphone to use.\n        \"\"\"\n        mics = self.list_available_mics()\n        if mic_name not in mics:\n            raise ValueError(f\"Microphone '{mic_name}' not found. Available microphones: {list(mics.keys())}\")\n        self.input_device = mics[mic_name]\n        print(f\"Changed input device to: {self.input_device}\")\n\n    def play_audio_file(self, file_path: str) -&gt; None:\n        \"\"\"\n        Play an audio file using the simpleaudio library.\n\n        Args:\n            file_path (str): Path to the audio file to play.\n        \"\"\"\n        AudioPlayback.play_audio_file(file_path)\n\n    def apply_noise_reduction_to_file(\n        self,\n        input_file: str,\n        output_file: str,\n        stationary: bool = False,\n        prop_decrease: float = 1.0,\n        n_std_thresh_stationary: float = 1.5,\n        n_jobs: int = 1,\n        use_torch: bool = False,\n        device: str = \"cuda\",\n    ) -&gt; None:\n        \"\"\"\n        Apply noise reduction to an audio file and save the result.\n\n        Args:\n            input_file (str): Path to the input audio file.\n            output_file (str): Path to save the processed audio file.\n            stationary (bool): Whether to perform stationary noise reduction.\n            prop_decrease (float): Proportion to reduce noise by (1.0 = 100%).\n            n_std_thresh_stationary (float): Threshold for stationary noise reduction.\n            n_jobs (int): Number of parallel jobs to run. Set to -1 to use all CPU cores.\n            use_torch (bool): Whether to use the PyTorch version of spectral gating.\n            device (str): Device to run the PyTorch spectral gating on (e.g., \"cuda\" or \"cpu\").\n        \"\"\"\n        AudioProcessing.apply_noise_reduction_to_file(\n            input_file,\n            output_file,\n            stationary=stationary,\n            prop_decrease=prop_decrease,\n            n_std_thresh_stationary=n_std_thresh_stationary,\n            n_jobs=n_jobs,\n            use_torch=use_torch,\n            device=device,\n        )\n\n    def _start_ffmpeg_process(self) -&gt; None:\n        \"\"\"Start the FFmpeg process for capturing live audio.\"\"\"\n        if self.process is not None:\n            return  # FFmpeg process is already running\n\n        # Calculate chunk size in bytes\n        bytes_per_sample = 4 if self.audio_format == \"f32le\" else 2  # 32-bit float or 16-bit int\n        self.chunk_size = int(self.sampling_rate * self.chunk_duration * self.channels * bytes_per_sample)\n\n        # FFmpeg command to capture live audio\n        command = [\n            \"ffmpeg\",\n            \"-f\", self.input_format,       # Input format (platform-specific)\n            \"-i\", self.input_device,       # Input device (platform-specific)\n            \"-ar\", str(self.sampling_rate),  # Sample rate\n            \"-ac\", str(self.channels),     # Number of channels\n            \"-f\", self.audio_format,      # Output format\n            \"-\"                           # Output to stdout\n        ]\n\n        try:\n            # Start FFmpeg process\n            self.process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n        except Exception as e:\n            raise RuntimeError(f\"Failed to start FFmpeg process: {e}\")\n\n    def stream_audio(self) -&gt; Generator[np.ndarray, None, None]:\n        \"\"\"Stream live audio from the microphone.\"\"\"\n        self._start_ffmpeg_process()\n        self.is_streaming = True\n\n        try:\n            while self.is_streaming:\n                # Read raw audio data from FFmpeg stdout\n                raw_data = self.process.stdout.read(self.chunk_size)\n                if not raw_data:\n                    # Print FFmpeg errors if no data is received\n                    stderr = self.process.stderr.read().decode()\n                    if stderr:\n                        print(\"FFmpeg errors:\", stderr)\n                    break\n\n                # Convert raw data to NumPy array\n                if self.audio_format == \"f32le\":\n                    audio_chunk = np.frombuffer(raw_data, dtype=np.float32)\n                elif self.audio_format == \"s16le\":\n                    audio_chunk = np.frombuffer(raw_data, dtype=np.int16) / 32768.0  # Normalize to [-1, 1]\n                else:\n                    raise RuntimeError(f\"Unsupported audio format: {self.audio_format}\")\n\n                # Yield the audio chunk for processing\n                yield audio_chunk\n\n        except KeyboardInterrupt:\n            print(\"\\nStreaming interrupted by user.\")\n        finally:\n            self.stop_streaming()\n\n    def stop_streaming(self) -&gt; None:\n        \"\"\"Stop the audio stream and terminate the FFmpeg process.\"\"\"\n        self.is_streaming = False\n        if self.process:\n            try:\n                self.process.terminate()\n                self.process.wait(timeout=5)  # Wait for the process to terminate\n            except subprocess.TimeoutExpired:\n                self.process.kill()  # Force kill if it doesn't terminate\n            self.process = None\n        print(\"Streaming stopped.\")\n\n    def save_recording(self, audio_data: np.ndarray, output_file: str, format: str = \"wav\") -&gt; None:\n        \"\"\"\n        Save the recorded audio to a file in the specified format.\n\n        Args:\n            audio_data (np.ndarray): The recorded audio data.\n            output_file (str): Path to save the recorded audio file.\n            format (str): Output format (e.g., \"wav\", \"mp3\", \"ogg\").\n        \"\"\"\n        # Scale the audio data to the appropriate range\n        if self.audio_format == \"f32le\":\n            # Scale floating-point data to the range [-1, 1]\n            audio_data = np.clip(audio_data, -1.0, 1.0)\n            # Convert to 16-bit integer format for saving\n            audio_data = (audio_data * 32767).astype(np.int16)\n        elif self.audio_format == \"s16le\":\n            # Data is already in 16-bit integer format\n            audio_data = audio_data.astype(np.int16)\n        else:\n            raise RuntimeError(f\"Unsupported audio format: {self.audio_format}\")\n\n        # Convert the NumPy array to a PyDub AudioSegment\n        audio_segment = AudioSegment(\n            audio_data.tobytes(),\n            frame_rate=self.sampling_rate,\n            sample_width=2,  # 16-bit audio (2 bytes per sample)\n            channels=self.channels,\n        )\n\n        # Normalize the volume to prevent clipping\n        audio_segment = audio_segment.normalize()\n\n        # Save the audio in the specified format\n        audio_segment.export(output_file, format=format)\n        print(f\"Recording saved to {output_file} in {format.upper()} format.\")\n\n    def process_audio_chunk(self, audio_chunk: np.ndarray, enable_noise_canceling: bool = True) -&gt; np.ndarray:\n        \"\"\"\n        Process an audio chunk with optional noise cancellation.\n\n        Args:\n            audio_chunk (np.ndarray): The audio chunk to process.\n            enable_noise_canceling (bool): Whether to apply noise cancellation.\n\n        Returns:\n            np.ndarray: The processed audio chunk.\n        \"\"\"\n        if enable_noise_canceling:\n            # Apply noise reduction using noisereduce\n            audio_chunk = AudioNoiseReduction.apply_noise_reduction(\n                audio_chunk,\n                self.sampling_rate,\n                stationary=self.stationary_noise_reduction,\n                prop_decrease=self.prop_decrease,\n                n_std_thresh_stationary=self.n_std_thresh_stationary,\n                n_jobs=self.n_jobs,  # Pass the number of parallel jobs\n                use_torch=self.use_torch,  # Enable/disable PyTorch\n                device=self.device,  # Specify the device for PyTorch\n            )\n            # Apply low-pass filter\n            audio_chunk = AudioNoiseReduction.apply_low_pass_filter(audio_chunk, self.sampling_rate, self.low_pass_cutoff)\n        return audio_chunk\n\n    def listen_and_record_with_vad(\n        self,\n        output_file: str = \"output.wav\",\n        silence_duration: float = 2.0,\n        format: str = \"wav\"\n    ) -&gt; None:\n        \"\"\"\n        Continuously listen to the microphone and record speech segments.\n\n        Args:\n            output_file (str): Path to save the recorded audio file.\n            silence_duration (float): Duration of silence (in seconds) to stop recording.\n            format (str): Output format (e.g., \"wav\", \"mp3\", \"ogg\").\n        \"\"\"\n        speech_segments: List[np.ndarray] = []\n        self.is_recording = False\n        silent_frames = 0\n        silence_threshold_frames = int(silence_duration / self.chunk_duration)\n\n        try:\n            for audio_chunk in self.stream_audio():\n                # Process the audio chunk with optional noise cancellation\n                processed_chunk = self.process_audio_chunk(audio_chunk, self.enable_noise_canceling)\n\n                # Process the audio chunk with VAD\n                is_speech = self.vad.process_audio(processed_chunk)\n\n                if is_speech:\n                    # Speech detected\n                    if not self.is_recording:\n                        print(\"\\nStarting recording...\")\n                        self.is_recording = True\n                        AudioPlayback.play_beep(600, 200)  # High-pitched beep for start\n                    speech_segments.append(processed_chunk)\n                    silent_frames = 0  # Reset silence counter\n                else:\n                    # Silence detected\n                    if self.is_recording:\n                        silent_frames += 1\n                        if silent_frames &gt;= silence_threshold_frames:\n                            # Stop recording if silence exceeds the threshold\n                            print(\"Stopping recording due to silence.\")\n                            self.is_recording = False\n                            AudioPlayback.play_beep(300, 200)  # Low-pitched beep for stop (async)\n\n                            # Save the recorded speech segment\n                            if speech_segments:\n                                combined_audio = np.concatenate(speech_segments)\n                                self.save_recording(combined_audio, output_file, format=format)\n                                speech_segments = []  # Reset for the next segment\n                        else:\n                            # Add silence to the current recording\n                            speech_segments.append(processed_chunk)\n\n        except KeyboardInterrupt:\n            print(\"\\nContinuous listening interrupted by user.\")\n\n        # Save any remaining speech segments\n        if speech_segments:\n            combined_audio = np.concatenate(speech_segments)\n            self.save_recording(combined_audio, output_file, format=format)\n\n    def stop(self):\n        \"\"\"Stop both streaming and recording.\"\"\"\n        self.stop_streaming()\n\n        # Stop the recording process.\n        self.is_recording = False\n        print(\"Recording stopped.\")\n</code></pre>"},{"location":"api/#live_audio_capture.LiveAudioCapture.__init__","title":"<code>__init__(sampling_rate=16000, chunk_duration=0.1, audio_format='f32le', channels=1, aggressiveness=1, enable_beep=True, enable_noise_canceling=False, low_pass_cutoff=7500.0, stationary_noise_reduction=False, prop_decrease=1.0, n_std_thresh_stationary=1.5, n_jobs=1, use_torch=False, device='cuda', calibration_duration=2.0, use_adaptive_threshold=True)</code>","text":"<p>Initialize the LiveAudioCapture instance.</p> <p>Parameters:</p> Name Type Description Default <code>sampling_rate</code> <code>int</code> <p>Sample rate in Hz (e.g., 16000).</p> <code>16000</code> <code>chunk_duration</code> <code>float</code> <p>Duration of each audio chunk in seconds (e.g., 0.1).</p> <code>0.1</code> <code>audio_format</code> <code>str</code> <p>Audio format for FFmpeg output (e.g., \"f32le\").</p> <code>'f32le'</code> <code>channels</code> <code>int</code> <p>Number of audio channels (1 for mono, 2 for stereo).</p> <code>1</code> <code>aggressiveness</code> <code>int</code> <p>Aggressiveness level for VAD (0 = least aggressive, 3 = most aggressive).</p> <code>1</code> <code>enable_beep</code> <code>bool</code> <p>Whether to play beep sounds when recording starts/stops.</p> <code>True</code> <code>enable_noise_canceling</code> <code>bool</code> <p>Whether to apply noise cancellation.</p> <code>False</code> <code>low_pass_cutoff</code> <code>float</code> <p>Cutoff frequency for the low-pass filter.</p> <code>7500.0</code> <code>stationary_noise_reduction</code> <code>bool</code> <p>Whether to use stationary noise reduction.</p> <code>False</code> <code>prop_decrease</code> <code>float</code> <p>Proportion to reduce noise by (1.0 = 100%).</p> <code>1.0</code> <code>n_std_thresh_stationary</code> <code>float</code> <p>Threshold for stationary noise reduction.</p> <code>1.5</code> <code>n_jobs</code> <code>int</code> <p>Number of parallel jobs to run. Set to -1 to use all CPU cores.</p> <code>1</code> <code>use_torch</code> <code>bool</code> <p>Whether to use the PyTorch version of spectral gating.</p> <code>False</code> <code>device</code> <code>str</code> <p>Device to run the PyTorch spectral gating on (e.g., \"cuda\" or \"cpu\").</p> <code>'cuda'</code> <code>calibration_duration</code> <code>float</code> <p>Duration of the calibration phase in seconds.</p> <code>2.0</code> <code>use_adaptive_threshold</code> <code>bool</code> <p>Whether to use adaptive thresholding for VAD.</p> <code>True</code> Source code in <code>live_audio_capture\\audio_capture.py</code> <pre><code>def __init__(\n    self,\n    sampling_rate: int = 16000,\n    chunk_duration: float = 0.1,\n    audio_format: str = \"f32le\",\n    channels: int = 1,\n    aggressiveness: int = 1,  # Aggressiveness level for VAD\n    enable_beep: bool = True,\n    enable_noise_canceling: bool = False,\n    low_pass_cutoff: float = 7500.0,\n    stationary_noise_reduction: bool = False,\n    prop_decrease: float = 1.0,\n    n_std_thresh_stationary: float = 1.5,\n    n_jobs: int = 1,\n    use_torch: bool = False,\n    device: str = \"cuda\",\n    calibration_duration: float = 2.0,  # Duration of calibration in seconds\n    use_adaptive_threshold: bool = True,  # Enable adaptive thresholding\n):\n    \"\"\"\n    Initialize the LiveAudioCapture instance.\n\n    Args:\n        sampling_rate (int): Sample rate in Hz (e.g., 16000).\n        chunk_duration (float): Duration of each audio chunk in seconds (e.g., 0.1).\n        audio_format (str): Audio format for FFmpeg output (e.g., \"f32le\").\n        channels (int): Number of audio channels (1 for mono, 2 for stereo).\n        aggressiveness (int): Aggressiveness level for VAD (0 = least aggressive, 3 = most aggressive).\n        enable_beep (bool): Whether to play beep sounds when recording starts/stops.\n        enable_noise_canceling (bool): Whether to apply noise cancellation.\n        low_pass_cutoff (float): Cutoff frequency for the low-pass filter.\n        stationary_noise_reduction (bool): Whether to use stationary noise reduction.\n        prop_decrease (float): Proportion to reduce noise by (1.0 = 100%).\n        n_std_thresh_stationary (float): Threshold for stationary noise reduction.\n        n_jobs (int): Number of parallel jobs to run. Set to -1 to use all CPU cores.\n        use_torch (bool): Whether to use the PyTorch version of spectral gating.\n        device (str): Device to run the PyTorch spectral gating on (e.g., \"cuda\" or \"cpu\").\n        calibration_duration (float): Duration of the calibration phase in seconds.\n        use_adaptive_threshold (bool): Whether to use adaptive thresholding for VAD.\n    \"\"\"\n    self.sampling_rate = sampling_rate\n    self.chunk_duration = chunk_duration\n    self.audio_format = audio_format\n    self.channels = channels\n    self.enable_beep = enable_beep\n    self.enable_noise_canceling = enable_noise_canceling\n    self.low_pass_cutoff = low_pass_cutoff\n    self.stationary_noise_reduction = stationary_noise_reduction\n    self.prop_decrease = prop_decrease\n    self.n_std_thresh_stationary = n_std_thresh_stationary\n    self.n_jobs = n_jobs\n    self.use_torch = use_torch\n    self.device = device\n    self.calibration_duration = calibration_duration\n    self.use_adaptive_threshold = use_adaptive_threshold\n    self.process: Optional[subprocess.Popen] = None\n    self.is_streaming = False\n    self.is_recording = False\n\n    # Validate the cutoff frequency\n    nyquist = 0.5 * self.sampling_rate\n    if self.low_pass_cutoff &gt;= nyquist:\n        raise ValueError(\n            f\"Cutoff frequency must be less than the Nyquist frequency ({nyquist} Hz). \"\n            f\"Provided cutoff frequency: {self.low_pass_cutoff} Hz.\"\n        )\n\n    # Initialize VAD\n    self.vad = VoiceActivityDetector(\n        sample_rate=sampling_rate,\n        frame_duration=chunk_duration,\n        aggressiveness=aggressiveness,\n        hysteresis_high=1.5,\n        hysteresis_low=0.5,\n        enable_noise_canceling=self.enable_noise_canceling,\n        calibration_duration=self.calibration_duration,\n        use_adaptive_threshold=self.use_adaptive_threshold,\n        audio_format=self.audio_format,\n        channels=self.channels\n    )\n\n    # Determine the input device based on the platform\n    if sys.platform == \"linux\":\n        self.input_format = \"alsa\"\n    elif sys.platform == \"darwin\":  # macOS\n        self.input_format = \"avfoundation\"\n    elif sys.platform == \"win32\":\n        self.input_format = \"dshow\"\n    else:\n        raise RuntimeError(f\"Unsupported platform: {sys.platform}\")\n\n    # Get the default microphone\n    self.input_device = MicUtils.get_default_mic()\n    print(f\"Using input device: {self.input_device}\")\n</code></pre>"},{"location":"api/#live_audio_capture.LiveAudioCapture.apply_noise_reduction_to_file","title":"<code>apply_noise_reduction_to_file(input_file, output_file, stationary=False, prop_decrease=1.0, n_std_thresh_stationary=1.5, n_jobs=1, use_torch=False, device='cuda')</code>","text":"<p>Apply noise reduction to an audio file and save the result.</p> <p>Parameters:</p> Name Type Description Default <code>input_file</code> <code>str</code> <p>Path to the input audio file.</p> required <code>output_file</code> <code>str</code> <p>Path to save the processed audio file.</p> required <code>stationary</code> <code>bool</code> <p>Whether to perform stationary noise reduction.</p> <code>False</code> <code>prop_decrease</code> <code>float</code> <p>Proportion to reduce noise by (1.0 = 100%).</p> <code>1.0</code> <code>n_std_thresh_stationary</code> <code>float</code> <p>Threshold for stationary noise reduction.</p> <code>1.5</code> <code>n_jobs</code> <code>int</code> <p>Number of parallel jobs to run. Set to -1 to use all CPU cores.</p> <code>1</code> <code>use_torch</code> <code>bool</code> <p>Whether to use the PyTorch version of spectral gating.</p> <code>False</code> <code>device</code> <code>str</code> <p>Device to run the PyTorch spectral gating on (e.g., \"cuda\" or \"cpu\").</p> <code>'cuda'</code> Source code in <code>live_audio_capture\\audio_capture.py</code> <pre><code>def apply_noise_reduction_to_file(\n    self,\n    input_file: str,\n    output_file: str,\n    stationary: bool = False,\n    prop_decrease: float = 1.0,\n    n_std_thresh_stationary: float = 1.5,\n    n_jobs: int = 1,\n    use_torch: bool = False,\n    device: str = \"cuda\",\n) -&gt; None:\n    \"\"\"\n    Apply noise reduction to an audio file and save the result.\n\n    Args:\n        input_file (str): Path to the input audio file.\n        output_file (str): Path to save the processed audio file.\n        stationary (bool): Whether to perform stationary noise reduction.\n        prop_decrease (float): Proportion to reduce noise by (1.0 = 100%).\n        n_std_thresh_stationary (float): Threshold for stationary noise reduction.\n        n_jobs (int): Number of parallel jobs to run. Set to -1 to use all CPU cores.\n        use_torch (bool): Whether to use the PyTorch version of spectral gating.\n        device (str): Device to run the PyTorch spectral gating on (e.g., \"cuda\" or \"cpu\").\n    \"\"\"\n    AudioProcessing.apply_noise_reduction_to_file(\n        input_file,\n        output_file,\n        stationary=stationary,\n        prop_decrease=prop_decrease,\n        n_std_thresh_stationary=n_std_thresh_stationary,\n        n_jobs=n_jobs,\n        use_torch=use_torch,\n        device=device,\n    )\n</code></pre>"},{"location":"api/#live_audio_capture.LiveAudioCapture.change_input_device","title":"<code>change_input_device(mic_name)</code>","text":"<p>Change the input device to the specified microphone by name.</p> <p>Parameters:</p> Name Type Description Default <code>mic_name</code> <code>str</code> <p>The name of the microphone to use.</p> required Source code in <code>live_audio_capture\\audio_capture.py</code> <pre><code>def change_input_device(self, mic_name: str) -&gt; None:\n    \"\"\"\n    Change the input device to the specified microphone by name.\n\n    Args:\n        mic_name (str): The name of the microphone to use.\n    \"\"\"\n    mics = self.list_available_mics()\n    if mic_name not in mics:\n        raise ValueError(f\"Microphone '{mic_name}' not found. Available microphones: {list(mics.keys())}\")\n    self.input_device = mics[mic_name]\n    print(f\"Changed input device to: {self.input_device}\")\n</code></pre>"},{"location":"api/#live_audio_capture.LiveAudioCapture.list_available_mics","title":"<code>list_available_mics()</code>","text":"<p>List all available microphones on the system.</p> <p>Returns:</p> Type Description <code>Dict[str, str]</code> <p>Dict[str, str]: A dictionary mapping microphone names to their device IDs.</p> Source code in <code>live_audio_capture\\audio_capture.py</code> <pre><code>def list_available_mics(self) -&gt; Dict[str, str]:\n    \"\"\"\n    List all available microphones on the system.\n\n    Returns:\n        Dict[str, str]: A dictionary mapping microphone names to their device IDs.\n    \"\"\"\n    return MicUtils.list_mics()\n</code></pre>"},{"location":"api/#live_audio_capture.LiveAudioCapture.listen_and_record_with_vad","title":"<code>listen_and_record_with_vad(output_file='output.wav', silence_duration=2.0, format='wav')</code>","text":"<p>Continuously listen to the microphone and record speech segments.</p> <p>Parameters:</p> Name Type Description Default <code>output_file</code> <code>str</code> <p>Path to save the recorded audio file.</p> <code>'output.wav'</code> <code>silence_duration</code> <code>float</code> <p>Duration of silence (in seconds) to stop recording.</p> <code>2.0</code> <code>format</code> <code>str</code> <p>Output format (e.g., \"wav\", \"mp3\", \"ogg\").</p> <code>'wav'</code> Source code in <code>live_audio_capture\\audio_capture.py</code> <pre><code>def listen_and_record_with_vad(\n    self,\n    output_file: str = \"output.wav\",\n    silence_duration: float = 2.0,\n    format: str = \"wav\"\n) -&gt; None:\n    \"\"\"\n    Continuously listen to the microphone and record speech segments.\n\n    Args:\n        output_file (str): Path to save the recorded audio file.\n        silence_duration (float): Duration of silence (in seconds) to stop recording.\n        format (str): Output format (e.g., \"wav\", \"mp3\", \"ogg\").\n    \"\"\"\n    speech_segments: List[np.ndarray] = []\n    self.is_recording = False\n    silent_frames = 0\n    silence_threshold_frames = int(silence_duration / self.chunk_duration)\n\n    try:\n        for audio_chunk in self.stream_audio():\n            # Process the audio chunk with optional noise cancellation\n            processed_chunk = self.process_audio_chunk(audio_chunk, self.enable_noise_canceling)\n\n            # Process the audio chunk with VAD\n            is_speech = self.vad.process_audio(processed_chunk)\n\n            if is_speech:\n                # Speech detected\n                if not self.is_recording:\n                    print(\"\\nStarting recording...\")\n                    self.is_recording = True\n                    AudioPlayback.play_beep(600, 200)  # High-pitched beep for start\n                speech_segments.append(processed_chunk)\n                silent_frames = 0  # Reset silence counter\n            else:\n                # Silence detected\n                if self.is_recording:\n                    silent_frames += 1\n                    if silent_frames &gt;= silence_threshold_frames:\n                        # Stop recording if silence exceeds the threshold\n                        print(\"Stopping recording due to silence.\")\n                        self.is_recording = False\n                        AudioPlayback.play_beep(300, 200)  # Low-pitched beep for stop (async)\n\n                        # Save the recorded speech segment\n                        if speech_segments:\n                            combined_audio = np.concatenate(speech_segments)\n                            self.save_recording(combined_audio, output_file, format=format)\n                            speech_segments = []  # Reset for the next segment\n                    else:\n                        # Add silence to the current recording\n                        speech_segments.append(processed_chunk)\n\n    except KeyboardInterrupt:\n        print(\"\\nContinuous listening interrupted by user.\")\n\n    # Save any remaining speech segments\n    if speech_segments:\n        combined_audio = np.concatenate(speech_segments)\n        self.save_recording(combined_audio, output_file, format=format)\n</code></pre>"},{"location":"api/#live_audio_capture.LiveAudioCapture.play_audio_file","title":"<code>play_audio_file(file_path)</code>","text":"<p>Play an audio file using the simpleaudio library.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>Path to the audio file to play.</p> required Source code in <code>live_audio_capture\\audio_capture.py</code> <pre><code>def play_audio_file(self, file_path: str) -&gt; None:\n    \"\"\"\n    Play an audio file using the simpleaudio library.\n\n    Args:\n        file_path (str): Path to the audio file to play.\n    \"\"\"\n    AudioPlayback.play_audio_file(file_path)\n</code></pre>"},{"location":"api/#live_audio_capture.LiveAudioCapture.process_audio_chunk","title":"<code>process_audio_chunk(audio_chunk, enable_noise_canceling=True)</code>","text":"<p>Process an audio chunk with optional noise cancellation.</p> <p>Parameters:</p> Name Type Description Default <code>audio_chunk</code> <code>ndarray</code> <p>The audio chunk to process.</p> required <code>enable_noise_canceling</code> <code>bool</code> <p>Whether to apply noise cancellation.</p> <code>True</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: The processed audio chunk.</p> Source code in <code>live_audio_capture\\audio_capture.py</code> <pre><code>def process_audio_chunk(self, audio_chunk: np.ndarray, enable_noise_canceling: bool = True) -&gt; np.ndarray:\n    \"\"\"\n    Process an audio chunk with optional noise cancellation.\n\n    Args:\n        audio_chunk (np.ndarray): The audio chunk to process.\n        enable_noise_canceling (bool): Whether to apply noise cancellation.\n\n    Returns:\n        np.ndarray: The processed audio chunk.\n    \"\"\"\n    if enable_noise_canceling:\n        # Apply noise reduction using noisereduce\n        audio_chunk = AudioNoiseReduction.apply_noise_reduction(\n            audio_chunk,\n            self.sampling_rate,\n            stationary=self.stationary_noise_reduction,\n            prop_decrease=self.prop_decrease,\n            n_std_thresh_stationary=self.n_std_thresh_stationary,\n            n_jobs=self.n_jobs,  # Pass the number of parallel jobs\n            use_torch=self.use_torch,  # Enable/disable PyTorch\n            device=self.device,  # Specify the device for PyTorch\n        )\n        # Apply low-pass filter\n        audio_chunk = AudioNoiseReduction.apply_low_pass_filter(audio_chunk, self.sampling_rate, self.low_pass_cutoff)\n    return audio_chunk\n</code></pre>"},{"location":"api/#live_audio_capture.LiveAudioCapture.save_recording","title":"<code>save_recording(audio_data, output_file, format='wav')</code>","text":"<p>Save the recorded audio to a file in the specified format.</p> <p>Parameters:</p> Name Type Description Default <code>audio_data</code> <code>ndarray</code> <p>The recorded audio data.</p> required <code>output_file</code> <code>str</code> <p>Path to save the recorded audio file.</p> required <code>format</code> <code>str</code> <p>Output format (e.g., \"wav\", \"mp3\", \"ogg\").</p> <code>'wav'</code> Source code in <code>live_audio_capture\\audio_capture.py</code> <pre><code>def save_recording(self, audio_data: np.ndarray, output_file: str, format: str = \"wav\") -&gt; None:\n    \"\"\"\n    Save the recorded audio to a file in the specified format.\n\n    Args:\n        audio_data (np.ndarray): The recorded audio data.\n        output_file (str): Path to save the recorded audio file.\n        format (str): Output format (e.g., \"wav\", \"mp3\", \"ogg\").\n    \"\"\"\n    # Scale the audio data to the appropriate range\n    if self.audio_format == \"f32le\":\n        # Scale floating-point data to the range [-1, 1]\n        audio_data = np.clip(audio_data, -1.0, 1.0)\n        # Convert to 16-bit integer format for saving\n        audio_data = (audio_data * 32767).astype(np.int16)\n    elif self.audio_format == \"s16le\":\n        # Data is already in 16-bit integer format\n        audio_data = audio_data.astype(np.int16)\n    else:\n        raise RuntimeError(f\"Unsupported audio format: {self.audio_format}\")\n\n    # Convert the NumPy array to a PyDub AudioSegment\n    audio_segment = AudioSegment(\n        audio_data.tobytes(),\n        frame_rate=self.sampling_rate,\n        sample_width=2,  # 16-bit audio (2 bytes per sample)\n        channels=self.channels,\n    )\n\n    # Normalize the volume to prevent clipping\n    audio_segment = audio_segment.normalize()\n\n    # Save the audio in the specified format\n    audio_segment.export(output_file, format=format)\n    print(f\"Recording saved to {output_file} in {format.upper()} format.\")\n</code></pre>"},{"location":"api/#live_audio_capture.LiveAudioCapture.stop","title":"<code>stop()</code>","text":"<p>Stop both streaming and recording.</p> Source code in <code>live_audio_capture\\audio_capture.py</code> <pre><code>def stop(self):\n    \"\"\"Stop both streaming and recording.\"\"\"\n    self.stop_streaming()\n\n    # Stop the recording process.\n    self.is_recording = False\n    print(\"Recording stopped.\")\n</code></pre>"},{"location":"api/#live_audio_capture.LiveAudioCapture.stop_streaming","title":"<code>stop_streaming()</code>","text":"<p>Stop the audio stream and terminate the FFmpeg process.</p> Source code in <code>live_audio_capture\\audio_capture.py</code> <pre><code>def stop_streaming(self) -&gt; None:\n    \"\"\"Stop the audio stream and terminate the FFmpeg process.\"\"\"\n    self.is_streaming = False\n    if self.process:\n        try:\n            self.process.terminate()\n            self.process.wait(timeout=5)  # Wait for the process to terminate\n        except subprocess.TimeoutExpired:\n            self.process.kill()  # Force kill if it doesn't terminate\n        self.process = None\n    print(\"Streaming stopped.\")\n</code></pre>"},{"location":"api/#live_audio_capture.LiveAudioCapture.stream_audio","title":"<code>stream_audio()</code>","text":"<p>Stream live audio from the microphone.</p> Source code in <code>live_audio_capture\\audio_capture.py</code> <pre><code>def stream_audio(self) -&gt; Generator[np.ndarray, None, None]:\n    \"\"\"Stream live audio from the microphone.\"\"\"\n    self._start_ffmpeg_process()\n    self.is_streaming = True\n\n    try:\n        while self.is_streaming:\n            # Read raw audio data from FFmpeg stdout\n            raw_data = self.process.stdout.read(self.chunk_size)\n            if not raw_data:\n                # Print FFmpeg errors if no data is received\n                stderr = self.process.stderr.read().decode()\n                if stderr:\n                    print(\"FFmpeg errors:\", stderr)\n                break\n\n            # Convert raw data to NumPy array\n            if self.audio_format == \"f32le\":\n                audio_chunk = np.frombuffer(raw_data, dtype=np.float32)\n            elif self.audio_format == \"s16le\":\n                audio_chunk = np.frombuffer(raw_data, dtype=np.int16) / 32768.0  # Normalize to [-1, 1]\n            else:\n                raise RuntimeError(f\"Unsupported audio format: {self.audio_format}\")\n\n            # Yield the audio chunk for processing\n            yield audio_chunk\n\n    except KeyboardInterrupt:\n        print(\"\\nStreaming interrupted by user.\")\n    finally:\n        self.stop_streaming()\n</code></pre>"},{"location":"api/#live_audio_capture.MicUtils","title":"<code>MicUtils</code>","text":"<p>Utilities for managing and interacting with microphones.</p> Source code in <code>live_audio_capture\\audio_utils\\mic_utils.py</code> <pre><code>class MicUtils:\n    \"\"\"\n    Utilities for managing and interacting with microphones.\n    \"\"\"\n\n    @staticmethod\n    def list_mics() -&gt; Dict[str, str]:\n        \"\"\"\n        List all available microphones on the system.\n\n        Returns:\n            Dict[str, str]: A dictionary mapping microphone names to their OS-specific device IDs.\n        \"\"\"\n        if sys.platform == \"linux\":\n            return MicUtils._list_mics_linux()\n        elif sys.platform == \"darwin\":  # macOS\n            return MicUtils._list_mics_mac()\n        elif sys.platform == \"win32\":\n            return MicUtils._list_mics_windows()\n        else:\n            raise RuntimeError(f\"Unsupported platform: {sys.platform}\")\n\n    @staticmethod\n    def _list_mics_linux() -&gt; Dict[str, str]:\n        \"\"\"List microphones on Linux using ALSA.\"\"\"\n        try:\n            result = subprocess.run([\"arecord\", \"-l\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True, encoding=\"utf-8\", errors=\"replace\")\n            if result.returncode != 0:\n                raise RuntimeError(\"Failed to list audio devices using arecord.\")\n\n            mics = {}\n            lines = result.stdout.splitlines()\n            for line in lines:\n                if \"card\" in line and \"device\" in line:\n                    match = re.search(r\"card (\\d+):.*device (\\d+):\", line)\n                    if match:\n                        card, device = match.groups()\n                        mic_name = f\"Card {card}, Device {device}\"\n                        mics[mic_name] = f\"hw:{card},{device}\"\n            return mics\n        except Exception as e:\n            print(f\"Error listing microphones: {e}\")\n            return {}\n\n    @staticmethod\n    def _list_mics_mac() -&gt; Dict[str, str]:\n        \"\"\"List microphones on macOS using AVFoundation.\"\"\"\n        try:\n            result = subprocess.run(\n                [\"ffmpeg\", \"-f\", \"avfoundation\", \"-list_devices\", \"true\", \"-i\", \"\"],\n                stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True, encoding=\"utf-8\", errors=\"replace\"\n            )\n            if result.returncode != 0:\n                raise RuntimeError(\"Failed to list audio devices using ffmpeg.\")\n\n            mics = {}\n            lines = result.stderr.splitlines()\n            for line in lines:\n                if \"AVFoundation audio devices\" in line:\n                    continue\n                if \"[AVFoundation input device\" in line and (\"Microphone\" in line or \"Built-in\" in line):\n                    match = re.search(r\"\\[(\\d+)\\]\", line)\n                    if match:\n                        device_id = match.group(1)\n                        mic_name = line.split(\"]\")[1].strip()\n                        mics[mic_name] = f\":{device_id}\"\n            return mics\n        except Exception as e:\n            print(f\"Error listing microphones: {e}\")\n            return {}\n\n    @staticmethod\n    def _list_mics_windows() -&gt; Dict[str, str]:\n        \"\"\"List microphones on Windows using DirectShow.\"\"\"\n        try:\n            result = subprocess.run(\n                [\"ffmpeg\", \"-f\", \"dshow\", \"-list_devices\", \"true\", \"-i\", \"dummy\"],\n                stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True, encoding=\"utf-8\", errors=\"replace\"\n            )\n            if result.returncode != 0:\n                raise RuntimeError(\"Failed to list audio devices using ffmpeg.\")\n\n            mics = {}\n            lines = result.stderr.splitlines()\n            for line in lines:\n                if \"DirectShow audio devices\" in line:\n                    continue\n                if \"microphone\" in line.lower() or \"Microphone Array\" in line:\n                    match = re.search(r'\"(.*)\"', line)\n                    if match:\n                        mic_name = match.group(1)\n                        mics[mic_name] = f\"audio={mic_name}\"\n            return mics\n        except Exception as e:\n            print(f\"Error listing microphones: {e}\")\n            return {}\n\n    @staticmethod\n    def get_default_mic() -&gt; str:\n        \"\"\"Get the default microphone device based on the platform.\"\"\"\n        if sys.platform == \"linux\":\n            return MicUtils._get_default_mic_linux()\n        elif sys.platform == \"darwin\":  # macOS\n            return MicUtils._get_default_mic_mac()\n        elif sys.platform == \"win32\":\n            return MicUtils._get_default_mic_windows()\n        else:\n            raise RuntimeError(f\"Unsupported platform: {sys.platform}\")\n\n    @staticmethod\n    def _get_default_mic_linux() -&gt; str:\n        \"\"\"Get the default microphone device on Linux using ALSA.\"\"\"\n        try:\n            result = subprocess.run([\"arecord\", \"-l\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True, encoding=\"utf-8\", errors=\"replace\")\n            if result.returncode != 0:\n                raise RuntimeError(\"Failed to list audio devices using arecord.\")\n\n            lines = result.stdout.splitlines()\n            for line in lines:\n                if \"card\" in line and \"device\" in line:\n                    match = re.search(r\"card (\\d+):.*device (\\d+):\", line)\n                    if match:\n                        card, device = match.groups()\n                        return f\"hw:{card},{device}\"\n        except Exception as e:\n            print(f\"Error detecting default microphone: {e}\")\n        return \"default\"  # Fallback to default\n\n    @staticmethod\n    def _get_default_mic_mac() -&gt; str:\n        \"\"\"Get the default microphone device on macOS using AVFoundation.\"\"\"\n        try:\n            result = subprocess.run(\n                [\"ffmpeg\", \"-f\", \"avfoundation\", \"-list_devices\", \"true\", \"-i\", \"\"],\n                stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True, encoding=\"utf-8\", errors=\"replace\"\n            )\n            if result.returncode != 0:\n                raise RuntimeError(\"Failed to list audio devices using ffmpeg.\")\n\n            lines = result.stderr.splitlines()\n            for line in lines:\n                if \"AVFoundation audio devices\" in line:\n                    continue\n                if \"[AVFoundation input device\" in line and \"Microphone\" in line:\n                    match = re.search(r\"\\[(\\d+)\\]\", line)\n                    if match:\n                        return f\":{match.group(1)}\"  # Format for macOS\n        except Exception as e:\n            print(f\"Error detecting default microphone: {e}\")\n        return \":0\"  # Fallback to default\n\n    @staticmethod\n    def _get_default_mic_windows() -&gt; str:\n        \"\"\"Get the default microphone device on Windows using DirectShow.\"\"\"\n        try:\n            result = subprocess.run(\n                [\"ffmpeg\", \"-f\", \"dshow\", \"-list_devices\", \"true\", \"-i\", \"dummy\"],\n                stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True, encoding=\"utf-8\", errors=\"replace\"\n            )\n            if result.returncode != 0:\n                raise RuntimeError(\"Failed to list audio devices using ffmpeg.\")\n\n            lines = result.stderr.splitlines()\n            for line in lines:\n                if \"DirectShow audio devices\" in line:\n                    continue\n                if \"microphone\" in line or \"Microphone\" in line or \"Microphone Array\" in line:\n                    match = re.search(r'\"(.*)\"', line)\n                    if match:\n                        return f\"audio={match.group(1)}\"  # Format for Windows\n        except Exception as e:\n            print(f\"Error detecting default microphone: {e}\")\n        return \"audio=Microphone\"  # Fallback to default\n</code></pre>"},{"location":"api/#live_audio_capture.MicUtils.get_default_mic","title":"<code>get_default_mic()</code>  <code>staticmethod</code>","text":"<p>Get the default microphone device based on the platform.</p> Source code in <code>live_audio_capture\\audio_utils\\mic_utils.py</code> <pre><code>@staticmethod\ndef get_default_mic() -&gt; str:\n    \"\"\"Get the default microphone device based on the platform.\"\"\"\n    if sys.platform == \"linux\":\n        return MicUtils._get_default_mic_linux()\n    elif sys.platform == \"darwin\":  # macOS\n        return MicUtils._get_default_mic_mac()\n    elif sys.platform == \"win32\":\n        return MicUtils._get_default_mic_windows()\n    else:\n        raise RuntimeError(f\"Unsupported platform: {sys.platform}\")\n</code></pre>"},{"location":"api/#live_audio_capture.MicUtils.list_mics","title":"<code>list_mics()</code>  <code>staticmethod</code>","text":"<p>List all available microphones on the system.</p> <p>Returns:</p> Type Description <code>Dict[str, str]</code> <p>Dict[str, str]: A dictionary mapping microphone names to their OS-specific device IDs.</p> Source code in <code>live_audio_capture\\audio_utils\\mic_utils.py</code> <pre><code>@staticmethod\ndef list_mics() -&gt; Dict[str, str]:\n    \"\"\"\n    List all available microphones on the system.\n\n    Returns:\n        Dict[str, str]: A dictionary mapping microphone names to their OS-specific device IDs.\n    \"\"\"\n    if sys.platform == \"linux\":\n        return MicUtils._list_mics_linux()\n    elif sys.platform == \"darwin\":  # macOS\n        return MicUtils._list_mics_mac()\n    elif sys.platform == \"win32\":\n        return MicUtils._list_mics_windows()\n    else:\n        raise RuntimeError(f\"Unsupported platform: {sys.platform}\")\n</code></pre>"},{"location":"api/#live_audio_capture.VoiceActivityDetector","title":"<code>VoiceActivityDetector</code>","text":"<p>A simplified voice activity detector (VAD) similar to WebRTC VAD. Features: - Energy-based speech detection. - Aggressiveness level to control detection strictness. - Hysteresis for stable speech detection.</p> Source code in <code>live_audio_capture\\vad.py</code> <pre><code>class VoiceActivityDetector:\n    \"\"\"\n    A simplified voice activity detector (VAD) similar to WebRTC VAD.\n    Features:\n    - Energy-based speech detection.\n    - Aggressiveness level to control detection strictness.\n    - Hysteresis for stable speech detection.\n    \"\"\"\n\n    def __init__(\n        self,\n        sample_rate: int = 16000,\n        frame_duration: float = 0.03,\n        aggressiveness: int = 1,  # Aggressiveness level (0, 1, 2, or 3)\n        hysteresis_high: float = 1.5,\n        hysteresis_low: float = 0.5,\n        enable_noise_canceling: bool = False,\n        calibration_duration: float = 2.0,  # Duration of calibration in seconds\n        use_adaptive_threshold: bool = True,  # Enable adaptive thresholding\n        audio_format: str = \"f32le\",  # Audio format for calibration\n        channels: int = 1\n    ):\n        \"\"\"\n        Initialize the VoiceActivityDetector.\n\n        Args:\n            sample_rate (int): Sample rate of the audio (default: 16000 Hz).\n            frame_duration (float): Duration of each frame in seconds (default: 0.03 seconds).\n            aggressiveness (int): Aggressiveness level (0 = least aggressive, 3 = most aggressive).\n            hysteresis_high (float): Multiplier for the threshold when speech is detected.\n            hysteresis_low (float): Multiplier for the threshold when speech is not detected.\n            enable_noise_canceling (bool): Whether to apply noise cancellation.\n            calibration_duration (float): Duration of the calibration phase in seconds.\n            use_adaptive_threshold (bool): Whether to use adaptive thresholding.\n            audio_format (str): Audio format for calibration (e.g., \"f32le\" or \"s16le\").\n            channels (int): Number of audio channels (1 for mono, 2 for stereo).\n        \"\"\"\n        self.sample_rate = sample_rate\n        self.frame_duration = frame_duration\n        self.frame_size = int(sample_rate * frame_duration)\n        self.aggressiveness = aggressiveness\n        self.hysteresis_high = hysteresis_high\n        self.hysteresis_low = hysteresis_low\n        self.enable_noise_canceling = enable_noise_canceling\n        self.calibration_duration = calibration_duration\n        self.use_adaptive_threshold = use_adaptive_threshold\n        self.audio_format = audio_format\n        self.channels = channels\n\n        # Calibrate the initial threshold\n        self.initial_threshold = self._calibrate_threshold() if self.use_adaptive_threshold else self._get_manual_threshold()\n        self.current_threshold = self.initial_threshold\n        self.speech_active = False\n\n        print(f\"Initialized VAD with aggressiveness={aggressiveness}, initial_threshold={self.initial_threshold:.6f}\")\n\n    def _calibrate_threshold(self) -&gt; float:\n        \"\"\"\n        Calibrate the initial energy threshold based on the background noise level.\n\n        Returns:\n            float: Calibrated initial energy threshold.\n        \"\"\"\n        print(\"Calibrating threshold... Please remain silent for a few seconds.\")\n        audio_chunks = self._capture_calibration_audio()\n        background_energy = np.mean([AudioProcessing.calculate_energy(chunk) for chunk in audio_chunks])\n        print(f\"Calibration complete. Background energy: {background_energy:.6f}\")\n\n        # Define multipliers based on aggressiveness\n        multipliers = {\n            0: 1.5,  # Least aggressive\n            1: 2.0,\n            2: 2.5,\n            3: 3.0,  # Most aggressive\n        }\n        return background_energy * multipliers.get(self.aggressiveness, 2.0)\n\n    def _capture_calibration_audio(self) -&gt; List[np.ndarray]:\n        \"\"\"\n        Capture a short audio sample for calibration.\n\n        Returns:\n            List[np.ndarray]: List of audio chunks captured during calibration.\n        \"\"\"\n        # Start FFmpeg process for calibration\n        command = [\n            \"ffmpeg\",\n            \"-f\", \"alsa\" if sys.platform == \"linux\" else \"avfoundation\" if sys.platform == \"darwin\" else \"dshow\",\n            \"-i\", MicUtils.get_default_mic(),\n            \"-ar\", str(self.sample_rate),\n            \"-ac\", str(self.channels),\n            \"-f\", self.audio_format,\n            \"-\"\n        ]\n        process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n\n        # Capture audio chunks for the calibration duration\n        audio_chunks = []\n        for _ in range(int(self.calibration_duration / self.frame_duration)):\n            raw_data = process.stdout.read(self.frame_size * 4)  # 4 bytes per sample for f32le\n            if not raw_data:\n                break\n            audio_chunk = AudioProcessing.process_audio_chunk(raw_data, self.audio_format)\n            audio_chunks.append(audio_chunk)\n\n        # Stop the FFmpeg process\n        process.terminate()\n        process.wait()\n\n        return audio_chunks\n\n    def _get_manual_threshold(self) -&gt; float:\n        \"\"\"\n        Get the initial energy threshold based on the aggressiveness level (manual values).\n\n        Returns:\n            float: Initial energy threshold.\n        \"\"\"\n        # Manual thresholds\n        if self.aggressiveness == 0:\n            return 0.0005 if not self.enable_noise_canceling else 0.00002  # Least aggressive (lowest threshold)\n        elif self.aggressiveness == 1:\n            return 0.001 if not self.enable_noise_canceling else 0.00003\n        elif self.aggressiveness == 2:\n            return 0.002 if not self.enable_noise_canceling else 0.00004\n        elif self.aggressiveness == 3:\n            return 0.005 if not self.enable_noise_canceling else 0.0001  # Most aggressive (highest threshold)\n        else:\n            raise ValueError(\"Aggressiveness must be between 0 and 3.\")\n\n    def process_audio(self, audio_chunk: np.ndarray) -&gt; bool:\n        \"\"\"\n        Process an audio chunk and determine if speech is detected.\n\n        Args:\n            audio_chunk (np.ndarray): Audio chunk to process.\n\n        Returns:\n            bool: True if speech is detected, False otherwise.\n        \"\"\"\n        # Calculate energy\n        energy = AudioProcessing.calculate_energy(audio_chunk)\n\n        # Detect speech based on energy\n        if energy &gt; self.current_threshold:\n            self.speech_active = True\n            print(\"Speech detected!\")\n        else:\n            self.speech_active = False\n            print(\"No speech detected.\")\n\n        # Update threshold using hysteresis\n        self._update_threshold()\n\n        # Debugging: Print key values\n        print(\n            f\"Energy: {energy:.6f}, Current Threshold: {self.current_threshold:.6f}, \"\n            f\"Speech Active: {self.speech_active}\"\n        )\n\n        return self.speech_active\n\n    def _update_threshold(self) -&gt; None:\n        \"\"\"\n        Update the energy threshold using hysteresis.\n        \"\"\"\n        if self.speech_active:\n            # Increase threshold slightly to avoid false positives\n            self.current_threshold = self.initial_threshold * self.hysteresis_high\n        else:\n            # Lower threshold to detect speech more sensitively\n            self.current_threshold = self.initial_threshold * self.hysteresis_low\n</code></pre>"},{"location":"api/#live_audio_capture.VoiceActivityDetector.__init__","title":"<code>__init__(sample_rate=16000, frame_duration=0.03, aggressiveness=1, hysteresis_high=1.5, hysteresis_low=0.5, enable_noise_canceling=False, calibration_duration=2.0, use_adaptive_threshold=True, audio_format='f32le', channels=1)</code>","text":"<p>Initialize the VoiceActivityDetector.</p> <p>Parameters:</p> Name Type Description Default <code>sample_rate</code> <code>int</code> <p>Sample rate of the audio (default: 16000 Hz).</p> <code>16000</code> <code>frame_duration</code> <code>float</code> <p>Duration of each frame in seconds (default: 0.03 seconds).</p> <code>0.03</code> <code>aggressiveness</code> <code>int</code> <p>Aggressiveness level (0 = least aggressive, 3 = most aggressive).</p> <code>1</code> <code>hysteresis_high</code> <code>float</code> <p>Multiplier for the threshold when speech is detected.</p> <code>1.5</code> <code>hysteresis_low</code> <code>float</code> <p>Multiplier for the threshold when speech is not detected.</p> <code>0.5</code> <code>enable_noise_canceling</code> <code>bool</code> <p>Whether to apply noise cancellation.</p> <code>False</code> <code>calibration_duration</code> <code>float</code> <p>Duration of the calibration phase in seconds.</p> <code>2.0</code> <code>use_adaptive_threshold</code> <code>bool</code> <p>Whether to use adaptive thresholding.</p> <code>True</code> <code>audio_format</code> <code>str</code> <p>Audio format for calibration (e.g., \"f32le\" or \"s16le\").</p> <code>'f32le'</code> <code>channels</code> <code>int</code> <p>Number of audio channels (1 for mono, 2 for stereo).</p> <code>1</code> Source code in <code>live_audio_capture\\vad.py</code> <pre><code>def __init__(\n    self,\n    sample_rate: int = 16000,\n    frame_duration: float = 0.03,\n    aggressiveness: int = 1,  # Aggressiveness level (0, 1, 2, or 3)\n    hysteresis_high: float = 1.5,\n    hysteresis_low: float = 0.5,\n    enable_noise_canceling: bool = False,\n    calibration_duration: float = 2.0,  # Duration of calibration in seconds\n    use_adaptive_threshold: bool = True,  # Enable adaptive thresholding\n    audio_format: str = \"f32le\",  # Audio format for calibration\n    channels: int = 1\n):\n    \"\"\"\n    Initialize the VoiceActivityDetector.\n\n    Args:\n        sample_rate (int): Sample rate of the audio (default: 16000 Hz).\n        frame_duration (float): Duration of each frame in seconds (default: 0.03 seconds).\n        aggressiveness (int): Aggressiveness level (0 = least aggressive, 3 = most aggressive).\n        hysteresis_high (float): Multiplier for the threshold when speech is detected.\n        hysteresis_low (float): Multiplier for the threshold when speech is not detected.\n        enable_noise_canceling (bool): Whether to apply noise cancellation.\n        calibration_duration (float): Duration of the calibration phase in seconds.\n        use_adaptive_threshold (bool): Whether to use adaptive thresholding.\n        audio_format (str): Audio format for calibration (e.g., \"f32le\" or \"s16le\").\n        channels (int): Number of audio channels (1 for mono, 2 for stereo).\n    \"\"\"\n    self.sample_rate = sample_rate\n    self.frame_duration = frame_duration\n    self.frame_size = int(sample_rate * frame_duration)\n    self.aggressiveness = aggressiveness\n    self.hysteresis_high = hysteresis_high\n    self.hysteresis_low = hysteresis_low\n    self.enable_noise_canceling = enable_noise_canceling\n    self.calibration_duration = calibration_duration\n    self.use_adaptive_threshold = use_adaptive_threshold\n    self.audio_format = audio_format\n    self.channels = channels\n\n    # Calibrate the initial threshold\n    self.initial_threshold = self._calibrate_threshold() if self.use_adaptive_threshold else self._get_manual_threshold()\n    self.current_threshold = self.initial_threshold\n    self.speech_active = False\n\n    print(f\"Initialized VAD with aggressiveness={aggressiveness}, initial_threshold={self.initial_threshold:.6f}\")\n</code></pre>"},{"location":"api/#live_audio_capture.VoiceActivityDetector.process_audio","title":"<code>process_audio(audio_chunk)</code>","text":"<p>Process an audio chunk and determine if speech is detected.</p> <p>Parameters:</p> Name Type Description Default <code>audio_chunk</code> <code>ndarray</code> <p>Audio chunk to process.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if speech is detected, False otherwise.</p> Source code in <code>live_audio_capture\\vad.py</code> <pre><code>def process_audio(self, audio_chunk: np.ndarray) -&gt; bool:\n    \"\"\"\n    Process an audio chunk and determine if speech is detected.\n\n    Args:\n        audio_chunk (np.ndarray): Audio chunk to process.\n\n    Returns:\n        bool: True if speech is detected, False otherwise.\n    \"\"\"\n    # Calculate energy\n    energy = AudioProcessing.calculate_energy(audio_chunk)\n\n    # Detect speech based on energy\n    if energy &gt; self.current_threshold:\n        self.speech_active = True\n        print(\"Speech detected!\")\n    else:\n        self.speech_active = False\n        print(\"No speech detected.\")\n\n    # Update threshold using hysteresis\n    self._update_threshold()\n\n    # Debugging: Print key values\n    print(\n        f\"Energy: {energy:.6f}, Current Threshold: {self.current_threshold:.6f}, \"\n        f\"Speech Active: {self.speech_active}\"\n    )\n\n    return self.speech_active\n</code></pre>"},{"location":"contributing/","title":"Contributing","text":"<p>Thank you for your interest in contributing to Live Audio Capture! We welcome contributions from everyone, whether you're fixing a bug, adding a feature, or improving documentation.</p>"},{"location":"contributing/#getting-started","title":"Getting Started","text":"<ol> <li>Fork the repository on GitHub.</li> <li>Clone your fork to your local machine:    <pre><code>git clone https://github.com/sami-rajichi/live_audio_capture.git\n</code></pre></li> <li>Create a new branch for your changes:    <pre><code>git checkout -b your-branch-name\n</code></pre></li> </ol>"},{"location":"contributing/#setting-up-the-development-environment","title":"Setting Up the Development Environment","text":"<ol> <li>Install Python: Ensure you have Python 3.9 or higher installed.</li> <li>Install Dependencies:    <pre><code>pip install -r requirements.txt\npip install -e .\n</code></pre></li> <li>Install FFmpeg: Follow the installation instructions in the Installation Guide.</li> </ol>"},{"location":"contributing/#making-changes","title":"Making Changes","text":"<ol> <li>Follow the Project Structure:</li> <li>Code lives in the <code>live_audio_capture/</code> directory.</li> <li>Tests are in the <code>tests/</code> directory.</li> <li>Examples are in the <code>examples/</code> directory.</li> <li>Write Clear and Concise Code: Ensure your code is easy to read and understand.</li> <li>Add Documentation: Update the relevant documentation if your changes introduce new features or modify existing behavior.</li> </ol>"},{"location":"contributing/#submitting-a-pull-request","title":"Submitting a Pull Request","text":"<ol> <li>Push Your Changes:    <pre><code>git push origin your-branch-name\n</code></pre></li> <li>Open a Pull Request:</li> <li>Go to the GitHub repository.</li> <li>Click New Pull Request.</li> <li>Select your branch and provide a clear description of your changes.</li> <li>Address Feedback: Be responsive to feedback and make necessary updates to your pull request.</li> </ol>"},{"location":"contributing/#thank-you","title":"Thank You!","text":"<p>Your contributions help make Live Audio Capture better for everyone. We appreciate your time and effort!</p>"},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.9 or higher: Ensure Python is installed on your system.</li> <li>FFmpeg: Required for audio file handling.</li> </ul>"},{"location":"installation/#install-ffmpeg","title":"Install FFmpeg","text":"<ul> <li> <p>Linux:   <pre><code>sudo apt update\nsudo apt install ffmpeg\n</code></pre></p> </li> <li> <p>macOS (using Homebrew):   <pre><code>brew install ffmpeg\n</code></pre></p> </li> <li> <p>Windows: Download FFmpeg from https://ffmpeg.org/download.html and add it to your system's <code>PATH</code>.</p> </li> </ul>"},{"location":"installation/#install-the-package","title":"Install the Package","text":"<p>You can install the package via pip:</p> <pre><code>pip install live_audio_capture\n</code></pre>"},{"location":"installation/#verify-installation","title":"Verify Installation","text":"<p>To verify that the package is installed correctly, run the following command:</p> <pre><code>pip show live_audio_capture\n</code></pre>"},{"location":"installation/#troubleshooting","title":"Troubleshooting","text":"<ul> <li>ModuleNotFoundError: If you encounter a <code>ModuleNotFoundError</code>, ensure that the package is installed in the correct Python environment.</li> <li>FFmpeg Not Found: If FFmpeg is not found, ensure it is installed and added to your system's <code>PATH</code>.</li> </ul>"},{"location":"usage/","title":"Usage","text":""},{"location":"usage/#basic-example","title":"Basic Example","text":"<p>Capture audio with voice activity detection and save it to a file:</p> <pre><code>from live_audio_capture import LiveAudioCapture\n\n# Initialize the audio capture\ncapture = LiveAudioCapture(\n    sampling_rate=16000,  # Sample rate in Hz\n    chunk_duration=0.3,   # Duration of each audio chunk in seconds\n    audio_format=\"f32le\",  # Audio format\n    channels=1,           # Mono audio\n    aggressiveness=1,     # VAD aggressiveness level\n    enable_beep=True,     # Play beep sounds when recording starts/stops\n    enable_noise_canceling=True,  # Enable noise cancellation\n    low_pass_cutoff=7500.0,       # Low-pass filter cutoff frequency\n    stationary_noise_reduction=True,  # Enable stationary noise reduction\n    prop_decrease=1.0,    # Proportion to reduce noise by\n    n_std_thresh_stationary=1.5,  # Threshold for stationary noise reduction\n    n_jobs=1,             # Number of parallel jobs\n    use_torch=False,      # Disable PyTorch for noise reduction\n    device=\"cpu\",         # Use CPU for noise reduction\n    calibration_duration=2.0,  # Calibration duration in seconds\n    use_adaptive_threshold=True,  # Enable adaptive thresholding for VAD\n)\n\n# Start recording with VAD\ncapture.listen_and_record_with_vad(\n    output_file=\"output.wav\",  # Save the recording to this file\n    silence_duration=2.0,      # Stop recording after 2 seconds of silence\n    format=\"wav\",              # Output format\n)\n</code></pre>"},{"location":"usage/#real-time-visualization","title":"Real-Time Visualization","text":"<p>Visualize audio in real-time:</p> <pre><code>import time\nfrom live_audio_capture import LiveAudioCapture, AudioVisualizer\nimport threading\n\n# Initialize the audio capture\ncapture = LiveAudioCapture(\n    sampling_rate=44100,  # Higher sample rate for better visualization\n    chunk_duration=0.1,\n    audio_format=\"f32le\",\n    channels=1,\n    enable_noise_canceling=False,  # Disable noise cancellation\n)\n\n# Initialize the audio visualizer\nvisualizer = AudioVisualizer(\n    sampling_rate=44100,\n    chunk_duration=0.1,\n)\n\n\n# Function to stream audio and visualize it\ndef stream_and_visualize():\n    for audio_chunk in capture.stream_audio():\n        visualizer.add_audio_chunk(audio_chunk)\n\n\n# Start the visualization in a separate thread\nvisualizer_thread = threading.Thread(target=stream_and_visualize)\nvisualizer_thread.start()\n\n# Let the visualization run for 10 seconds\ntime.sleep(10)\n\n# Stop the capture and visualization\ncapture.stop()\nvisualizer.stop()\n</code></pre>"},{"location":"usage/#advanced-example","title":"Advanced Example","text":"<p>Use the real-time audio capture along with the voice activity detector within a thread (It's highly recommended to follow this approach):</p> <pre><code>from live_audio_capture import LiveAudioCapture\nimport threading\nimport time\n\n# Initialize the audio capture\ncapture = LiveAudioCapture(\n    sampling_rate=16000,\n    chunk_duration=0.3,\n    audio_format=\"f32le\",\n    channels=1,\n    aggressiveness=1,\n    enable_beep=True,\n    enable_noise_canceling=False,\n    low_pass_cutoff=7500.0,\n    stationary_noise_reduction=True,\n    prop_decrease=1.0,\n    n_std_thresh_stationary=1.5,\n    n_jobs=1,\n    use_torch=False,\n    device=\"cpu\",\n    calibration_duration=2.0,\n    use_adaptive_threshold=True,\n)\n\n\n# Function to start recording with VAD\ndef record_with_vad():\n    capture.listen_and_record_with_vad(\n        output_file=\"output.wav\",\n        silence_duration=2.0,\n        format=\"wav\"\n    )\n\n\n# Start the recording in a separate thread\nrecording_thread = threading.Thread(target=record_with_vad)\nrecording_thread.start()\n\n# Let the recording run for 10 seconds\ntime.sleep(10)\n\n# Stop the recording\ncapture.stop()\n\nprint(\"Recording stopped.\")\n</code></pre>"},{"location":"usage/#noise-reduction-example","title":"Noise Reduction Example","text":"<p>Apply the noise reducer feature on a noisy-environment recorded sample:</p> <pre><code>from live_audio_capture.audio_utils import AudioProcessing, AudioPlayback\n\n# Apply noise reduction to a pre-recorded file\nAudioProcessing.apply_noise_reduction_to_file(\n    input_file=\"input.wav\",  # Path to the input file\n    output_file=\"output.wav\",  # Path to save the processed file\n    stationary=True,  # Enable stationary noise reduction\n    prop_decrease=1.0,  # Reduce noise by 100%\n    n_std_thresh_stationary=1.5,  # Threshold for stationary noise reduction\n    n_jobs=1,  # Use a single job for noise reduction\n    use_torch=False,  # Disable PyTorch for noise reduction\n    device=\"cpu\",  # Use CPU for noise reduction\n)\n\nAudioPlayback.play_audio_file(\"output.wav\")\n\nprint(\"Noise reduction applied and saved to output.wav.\")\n</code></pre>"},{"location":"usage/#change-input-device","title":"Change Input Device","text":"<p>Modify the microphone name if needed:</p> <pre><code>from live_audio_capture import LiveAudioCapture\n\n# Initialize the audio capture\ncapture = LiveAudioCapture(\n    sampling_rate=16000,\n    chunk_duration=0.1,\n    audio_format=\"f32le\",\n    channels=1,\n)\n\n# List available microphones\nmics = capture.list_available_mics()\nprint(\"Available microphones:\")\nfor mic_name, device_id in mics.items():\n    print(f\"{mic_name}: {device_id}\")\n\n# Change the input device to the first available microphone\nif mics:\n    first_mic_name = list(mics.keys())[0]\n    capture.change_input_device(first_mic_name)\n    print(f\"Changed input device to: {first_mic_name}\")\n\n# Start recording with VAD\ncapture.listen_and_record_with_vad(\n    output_file=\"output.wav\",\n    silence_duration=2.0,\n    format=\"wav\",\n)\n</code></pre>"}]}